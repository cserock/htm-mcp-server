{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCP + LangGraph Client Example\n",
    "\n",
    "**참고자료**\n",
    "- https://modelcontextprotocol.io/introduction\n",
    "- https://github.com/langchain-ai/langchain-mcp-adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경설정\n",
    "\n",
    "아래 설치 방법을 참고하여 `uv` 를 설치합니다.\n",
    "\n",
    "**uv 설치 방법**\n",
    "\n",
    "```bash\n",
    "# macOS/Linux\n",
    "curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# Windows (PowerShell)\n",
    "irm https://astral.sh/uv/install.ps1 | iex\n",
    "```\n",
    "\n",
    "**의존성 설치**\n",
    "\n",
    "```bash\n",
    "uv pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "환경변수를 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiServerMCPClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사전에 `mcp_server_remote.py` 를 실행해둡니다. 터미널을 열고 가상환경이 활성화 되어 있는 상태에서 서버를 실행해 주세요.\n",
    "\n",
    "> 명령어\n",
    "```bash\n",
    "source .venv/bin/activate\n",
    "python mcp_server_remote.py\n",
    "```\n",
    "\n",
    "`async with` 로 일시적인 Session 연결을 생성 후 해제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StructuredTool(name='search', description='\\n    Performs hybrid search (keyword + semantic) on MD documents.\\n    Combines exact keyword matching and semantic similarity to deliver optimal results.\\n    The most versatile search option for general questions or when unsure which search type is best.\\n    \\n    Parameters:\\n        query: Search query\\n        top_k: Number of results to return\\n\\n    ', args_schema={'properties': {'query': {'title': 'Query', 'type': 'string'}, 'top_k': {'default': 4, 'title': 'Top K', 'type': 'integer'}}, 'required': ['query'], 'title': 'searchArguments', 'type': 'object'}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x10ed5d440>)]\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36magent\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mtools\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "## Search Results\n",
      "\n",
      "### Result 1 (Page: 325)\n",
      "\n",
      "요17:21-23 아버지여, 아버지께서 내 안에, 내가 아버지 안에 있는 것 같이 그들도 다 하나가 되어 우리 안에 있게\n",
      "하사 세상으로 아버지께서 나를 보내신 것을 믿게 하옵소서 (22)내게 주신 영광을 내가 그들에게 주었사오니 이는 우\n",
      "리가 하나가 된 것 같이 그들도 하나가 되게 하려 함이니이다 (23)곧 내가 그들 안에 있고 아버지께서 내 안에 계시\n",
      "어 그들로 온전함을 이루어 하나가 되게 하려 함은 아버지께서 나를 보내신 것과 또 나를 사랑하심 같이 그들도 사랑\n",
      "하신 것을 세상으로 알게 하려 함이로소이다\n",
      "* 부르심의 자리와 영적 공동체에 대해 좀 더 알기 원하는 분은\n",
      "『킹덤빌더(규장)』책 참조\n",
      "\n",
      "Source: 4a414ed1-67ae-49f6-acf5-a92ad7a3c206_31_kbs_cleaned.pdf\n",
      "\n",
      "---\n",
      "\n",
      "### Result 2 (Page: 292)\n",
      "\n",
      "# Ⅳ. 킹덤 믿음체계와 멘탈리티\n",
      "1\n",
      "# 심중을 새롭게 하기(믿음체계 확립)\n",
      "약1:2 그러므로 모든 더러운 것과 넘치는 악을 내버리고 너희 영혼을 능히 구원할 바 마음(헬, 카르디아: 심중)에 심\n",
      "어진 말씀을 온유함으로 받으라\n",
      "약4:8 하나님을 가까이하라 그리하면 너희를 가까이하시리라 죄인들아 손을 깨끗이 하라 두 마음(헬, 뒵시코스: 두\n",
      "혼)을 품은 자들아 마음(헬, 카르디아: 심중)을 성결하게 하라\n",
      "# 킹덤 믿음체계 확립을 위한 7가지 알약 먹기\n",
      "나는 예수 그리스도 안에서\n",
      "1) 무한하신 하나님의 지혜와 능력을 누립니다.\n",
      "2) 항상 기뻐하고 범사에 감사합니다.\n",
      "3) 말할 수 없는 은혜와 평강을 누립니다.\n",
      "4) 언제나 인내를 통하여 온전함을 누립니다.\n",
      "5) 말씀대로 말함으로 주의 뜻을 이룹니다.\n",
      "6) 늘 신성한 강건함과 자유를 누립니다.\n",
      "7) 차고 넘치는 부요와 형통을 누립니다.\n",
      "2\n",
      "# 킹덤 멘탈리티\n",
      "# 1) 킹덤 멘탈리티란?\n",
      "킹덤 멘탈리티(Kingdom Mentality: 하나님나라의 사고방식)는 예수 그리스도 안에서 성령을 통하여 하\n",
      "나님 통치의 관점으로 보는 사고방식이다. 예수님께서 말씀하시고 가르쳐 주신 것들은 오늘날도 성령님\n",
      "을 통해서 알게 되고 배울 수 있지만, 킹덤 멘탈리티 교회의 교리와 전통으로 인하여 아직 제대로 수용하\n",
      "지 못하는 “성령님의 인도하심”과 “성령의 권능 안에서 말씀의 풀어짐”, “성령님의 나타나심과 현\n",
      "재적 역사”에 적극적으로 의지하는 사고방식이다. 또한, 하나님을 영화롭게 하고 하나님의 뜻을 이 땅\n",
      "\n",
      "Source: 4a414ed1-67ae-49f6-acf5-a92ad7a3c206_31_kbs_cleaned.pdf\n",
      "\n",
      "---\n",
      "\n",
      "### Result 3 (Page: 110)\n",
      "\n",
      "롬 8:9 만일 너희 속에 하나님의 영이 거하시면 너희가 육신에 있지 아니하고 영에 있나니 누구든지 그리스도의 영\n",
      "이 없으면 그리스도의 사람이 아니라\n",
      "“우리로 또한 새 생명 가운데서”\n",
      "새 생명(a new life)이란 단순한 새 생명이 아니라 생명의 질적인 변화를 의미(newness of life)한다. 이\n",
      "것은 새로운 피조물(고후5:17), 새 사람(엡4:24)에서도 동일하다. 이것은 우리의 타락한 영이 새로워지는\n",
      "것이 아니라 본질적으로 다른 그리스도의 생명으로 다시 태어난다는 뜻이다.\n",
      "따라서 “새 생명 가운데서 행한다”라는 의미는 세상에 묶여 있는 마음으로 형성된 자기의식이 내 삶\n",
      "의 주인이 아니고, 내 안에 계신 그리스도의 생명으로 말미암아 그리스도 의식을 가진 참 자아가 하나님\n",
      "의 영으로부터 주어지는 진리의 말씀으로 살아간다는 것이다.\n",
      "2\n",
      "# 예수 그리스도의 죽으심과 부활\n",
      "하나님의 아들이신 예수님이 지신 십자가는 죽음만을 의미하는 것이 아니라 동시에 부활을 의미한다. 십\n",
      "자가의 앞면이 육신으로 오신 예수님의 죽음이라면 십자가의 뒷면은 부활이다. 예수님의 죽으심과 부활\n",
      "은 나누어서 생각할 수 없고, 두 면을 동시에 생각할 때만 십자가의 도의 전부를 볼 수 있다. 예수님의 죽\n",
      "으심과 부활 중 한 면만 이야기하면 온전한 십자가의 메시지가 되지 못한다. 우리는 항상 십자가에 매달\n",
      "려 죽으셨을 뿐만 아니라, 죽음에서 부활하신 예수 그리스도를 마음에 품어야 한다.\n",
      "동전(coin)에는 항상 앞면과 뒷면이 있는 것처럼, 십자가의 도에는 예수 그리스도의 죽으심과 부활이 항상 함께\n",
      "한다. 진리는 하나이지만 양면성을 띠고 있다. 십자가의 죽으심과 부활을 함께 말하지 않는 복음은 왜곡\n",
      "된 복음이다.\n",
      "다음 성경의 말씀을 다시 묵상해 보자. (로마서 6장은 십자가의 장이라고 불린다.)\n",
      "롬 6:5 만일 우리가 그의 죽으심과 같은 모양으로 연합한 자가 되었으면 또한 그의 부활과 같은 모양으로 연합한 자\n",
      "도 되리라\n",
      "\n",
      "Source: 4a414ed1-67ae-49f6-acf5-a92ad7a3c206_31_kbs_cleaned.pdf\n",
      "\n",
      "---\n",
      "\n",
      "### Result 4 (Page: 97)\n",
      "\n",
      "![](./images/figure/4a414ed1-67ae-49f6-acf5-a92ad7a3c206_31_kbs_cleaned_Page_96_Index_1161.png)\n",
      "\n",
      "<image>\n",
      "<title>\n",
      "신체, 마음, 혼의 관계 구조\n",
      "</title>\n",
      "<details>\n",
      "이 이미지는 신체, 마음, 혼의 관계를 피라미드 형태로 나타내고 있으며, 각 요소의 상호작용과 그 의미를 설명하고 있다. 신체는 외부 세계에서의 행동 양식을 나타내고, 마음은 사고 체계와 관련이 있으며, 혼은 의식의 중심으로 기능한다. 또한, 심중은 신념 체계와 연결되어 있다.\n",
      "</details>\n",
      "<entities>\n",
      "신체, 마음, 혼, 심중, 의식\n",
      "</entities>\n",
      "<hypothetical_questions>\n",
      "- 신체와 마음의 관계는 개인의 행동에 어떤 영향을 미칠까?\n",
      "- 혼의 개념이 현대 심리학에서 어떻게 해석될 수 있을까?\n",
      "</hypothetical_questions>\n",
      "</image>\n",
      "\n",
      "Source: 4a414ed1-67ae-49f6-acf5-a92ad7a3c206_31_kbs_cleaned.pdf\n",
      "\n",
      "---\n",
      "\n",
      "### Result 5 (Page: 114)\n",
      "\n",
      "그런데 오늘날 그 십자가를 우상시하고 있다. 마치 십자가를 부적처럼 생각하고 있다. 이 방 저 방에 십\n",
      "자가를 걸고 목에다 손에다 십자가 형상을 지니고 있으면 또 십자가를 사랑하면, 모든 저주가 끊어지고\n",
      "모든 일이 잘될 것이라고 생각한다. 하지만, 이는 사실이 아니다. 오히려 당신은 속고 있는 것이며, 예수\n",
      "님의 죽으심을 헛되게 하고 있으며, 당신이 마귀의 표적이 되는 것이다.\n",
      "이런 일들은 과거 이스라엘 백성에게도 일어났다. 유대 히스기야왕 때 이스라엘 백성은 과거 모세의 놋\n",
      "뱀 사건을 기억했고, 시간이 지나자 그것을 우상숭배 했던 것과 똑같은 행동이 지금도 일어나고 있다.\n",
      "왕하 18:3-4 히스기야가 그의 조상 다윗의 모든 행위와 같이 여호와께서 보시기에 정직하게 행하여 (4) 그가 여러\n",
      "산당들을 제거하며 주상을 깨뜨리며 아세라 목상을 찍으며 모세가 만들었던 놋뱀을 이스라엘 자손이 이때까지 향하\n",
      "여 분향하므로 그것을 부수고 느후스단이라 일컬었더라\n",
      "십자가는 부적이 아니다. 십자가에 능력이 있는 것은 예수님의 죽으심으로 인한 피 흘림과 그 십자가에서\n",
      "옛사람의 죽음 때문에 당신 안에 그리스도가 나타나심에 있다는 사실을 알아야 한다.\n",
      "1) 내가 걸고 있는 십자가상의 예수님은 우리의 구원자이시고 살아계신 하나님의 아들이시다.\n",
      "2) 그분은 나 때문에 모든 죄를 짊어지시고 죽으시고, 율법의 저주가 되셨다.\n",
      "3) 나는 이 분과 연합하여 죽고 부활하였음으로 새로운 피조물로 거듭난 자이다.\n",
      "4) 나는 끊임없이 내 거짓자아를 부인하고, 예수 그리스도를 나타내는 삶을 사는 자이다.\n",
      "\n",
      "Source: 4a414ed1-67ae-49f6-acf5-a92ad7a3c206_31_kbs_cleaned.pdf\n",
      "\n",
      "---\n",
      "\n",
      "### Result 6 (Page: 288)\n",
      "\n",
      "믿음의 법칙, 수확의 법칙을 기억하라. 우리 모두는 각자가 믿음으로 심는 것을 거둔다. 내 안에 ‘없\n",
      "음’을 심으면 ‘없음’을 거두고, ‘있음’을 심으면 ‘있음’을 거둔다. 예수 그리스도를 통해 하나\n",
      "님 아버지께서 주신 모든 것이 이미 우리 안에 있다는 진리로, 즉 있음의식을 가지고 믿음으로 우리 마음\n",
      "에 심음으로써, 예수 그리스도께서 법적으로 이루신 모든 것을 현실에서도 누리는 우리가 되어야한다.\n",
      "갈 6:7 스스로 속이지 말라 하나님은 업신여김을 받지 아니하시나니 사람이 무엇으로 심든지 그대로 거두리라\n",
      "# (3) 성령 하나님의 갈망(창조)의식\n",
      "거짓자아의 의식은 공허함과 상실감으로 인해 평강을 누리지 못하며, 늘 의미있고 가치있는 것을 찾고\n",
      "있다. 이 공허함과 상실감이 커지게 되면, 우리는 존재 깊은 곳에서부터 수치심, 무가치함을 느낀다. 이\n",
      "러한 감정과 느낌을 덮고 상쇄하기 위해 우리는 끊임없이 외부의 것을 취하고 추구함으로, 지금 있는 그\n",
      "대로가 아닌 미래에 있는 그 무엇으로 살아있음을 느끼고자 한다. 참 만족과 평강 되시는 하나님 대신에\n",
      "보다 의미있고 가치있는 것을 찾기 위해, 그럼으로써 자신의 존재를 증명하기 위해 발버둥치며 살아간\n",
      "다는 것이다. 세상 모든 것을 창조하신 하나님의 말씀이자 창조주이시며 지금 이 순간에도 온 우주 만물\n",
      "을 붙들고 계시는 예수님을 추구하기 보다는, 언젠가는 사라질 눈에 보이는 물질세계에만 관심을 기울\n",
      "이며 관계하고 추구한다.\n",
      "히11:3 믿음으로 모든 세계가 하나님의 말씀으로 지어진 줄을 우리가 아나니 보이는 것은 나타난 것으로 말미암아\n",
      "된 것이 아니니라\n",
      "고후4:18 우리가 주목하는 것은 보이는 것이 아니요 보이지 않는 것이니 보이는 것은 잠깐이요 보이지 않는 것은\n",
      "영원함이라\n",
      "이 세상에서 가장 가치있는 일은 내가 알고 있는 것을 매일 삶을 통해서 체험함으로 자신의 존재를 증명\n",
      "하는 것이다. 우리 영은 하나님이 누구이신지, 내가 누구인지 왜 사는지 어떻게 살아야 하는지를 안다.\n",
      "\n",
      "Source: 4a414ed1-67ae-49f6-acf5-a92ad7a3c206_31_kbs_cleaned.pdf\n",
      "\n",
      "---\n",
      "\n",
      "### Result 7 (Page: 113)\n",
      "\n",
      "은 우리의 타락한 형상이라는 것을 보아야 하며, 4) 우리의 타락한 본성과 죄악된 실체가 예수 그리스도와\n",
      "연합함으로 마침내 그 십자가에서 죽은 것을 보아야 한다.\n",
      "# ▶십자가 목걸이의 진정한 의미\n",
      "많은 사람들이 십자가 목걸이나 문양을 하고 다닌다. 그것은 아마 ‘나는 예수님을 믿고 사랑합니다’\n",
      "라는 뜻일 것이다. 그러나 실제의 뜻은 ‘나는 죽을 수밖에 없는 마귀의 자식입니다. 예수님이 십자가에\n",
      "서 내 대신에 죄가 되고 저주를 받으사 뱀같이 되어 버린 것입니다. 사실 그 뱀은 바로 나입니다. 그런데\n",
      "예수님의 대속으로 인하여 내가 그 죄와 저주에 대해서 죽었습니다. 이제는 나는 나의 삶이 아니라 예수\n",
      "님의 삶을 사는 자입니다’라는 것을 의미한다. 복음은 나의 삶이 아닌 나의 죽음을 통한 그리스도의 삶\n",
      "에 대해 말하고 있다. 십자가가 얼마나 혐오스럽고 악한 것인지를 아는 자만이 십자가의 죽음을 통해 나\n",
      "타난 예수님의 사랑을 체험할 수 있다.\n",
      "\n",
      "Source: 4a414ed1-67ae-49f6-acf5-a92ad7a3c206_31_kbs_cleaned.pdf\n",
      "\n",
      "---\n",
      "\n",
      "### Result 8 (Page: 273)\n",
      "\n",
      "![](./images/figure/4a414ed1-67ae-49f6-acf5-a92ad7a3c206_31_kbs_cleaned_Page_272_Index_3194.png)\n",
      "\n",
      "<image>\n",
      "<title>\n",
      "거짓 자아와 세 차원의 삶의 구조\n",
      "</title>\n",
      "<details>\n",
      "이미지는 거짓 자아의 개념과 혼, 정신, 몸, 마음, 심중, 신체 간의 관계를 나타내고 있습니다. 이 구조는 믿음, 인식, 투사, 실상, 에너지의 흐름을 통해 세 차원의 삶을 설명합니다.\n",
      "</details>\n",
      "<entities>\n",
      "거짓 자아, 혼, 정신, 몸, 마음, 심중, 신체, 믿음, 인식, 투사, 실상, 에너지\n",
      "</entities>\n",
      "<hypothetical_questions>\n",
      "- 거짓 자아를 극복하기 위해 어떤 방법이 있을까요?\n",
      "- 세 차원의 삶을 경험하기 위해 필요한 믿음의 요소는 무엇인가요?\n",
      "</hypothetical_questions>\n",
      "</image>\n",
      "\n",
      "Source: 4a414ed1-67ae-49f6-acf5-a92ad7a3c206_31_kbs_cleaned.pdf\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36magent\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "심중(心中)은 마음의 중심, 즉 내면 깊은 곳에 자리한 마음의 핵심을 의미합니다. 심중은 단순한 감정이나 생각을 넘어서서 개인의 신념 체계, 의식의 중심, 그리고 영적 상태와 깊이 연결되어 있습니다. \n",
      "\n",
      "자료에 따르면 심중은 헬라어로 '카르디아'라고 하며, 믿음체계 확립과 영적 성장에 중요한 역할을 합니다. 예를 들어, 야고보서 1장 2절에서는 \"너희 영혼을 능히 구원할 바 마음(심중)에 심어진 말씀을 온유함으로 받으라\"고 하여 심중에 심어진 말씀이 영혼 구원에 영향을 준다고 말합니다. 또한, 야고보서 4장 8절에서는 \"마음(심중)을 성결하게 하라\"고 하여 심중의 정결함이 중요함을 강조합니다.\n",
      "\n",
      "심중은 신체, 마음, 혼과도 관계가 있으며, 신념 체계와 연결되어 개인의 행동과 사고에 영향을 미칩니다. 심중은 단순한 마음의 작용이 아니라, 영적이고 내면적인 깊은 차원의 마음 상태로서, 하나님과의 관계, 믿음, 그리고 영적 삶의 중심이 됩니다.\n",
      "\n",
      "요약하면, 심중은 내면 깊은 마음의 중심으로서, 신념과 영적 상태를 담고 있으며, 믿음과 영적 성장에 핵심적인 역할을 하는 개념입니다."
     ]
    }
   ],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from utils import ainvoke_graph, astream_graph\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model_name=\"gpt-4.1-mini\", temperature=0, max_tokens=20000\n",
    ")\n",
    "\n",
    "async with MultiServerMCPClient(\n",
    "    {\n",
    "        \"tbb-db-schema\": {\n",
    "            # 서버의 포트와 일치해야 합니다.(8005번 포트)\n",
    "            \"url\": \"http://localhost:8006/sse\",\n",
    "            # \"url\": \"http://localhost:8008/sse\",\n",
    "            \"transport\": \"sse\",\n",
    "        }\n",
    "    }\n",
    ") as client:\n",
    "    print(client.get_tools())\n",
    "    agent = create_react_agent(model, client.get_tools())\n",
    "    # answer = await astream_graph(agent, {\"messages\": \"서울의 날씨는 어떠니?\"})\n",
    "    # answer = await astream_graph(agent, {\"messages\": \"텀블벅 스키마\"})\n",
    "    # 현재시간을 구하는 MCP\n",
    "    # answer = await astream_graph(agent, {\"messages\": \"현재 서울 시간은 몇시인가요?\"})\n",
    "\n",
    "    # kbs rag mcp 테스트\n",
    "    answer = await astream_graph(agent, {\"messages\": \"심중에 대해 자세히 설명해 주세요.\"})\n",
    "    # answer = await astream_graph(agent, {\"messages\": \"십일조는 왜 해야하나요?\"})\n",
    "    # answer = await astream_graph(agent, {\"messages\": \"드림과 나눔 중 어느 것이 더 중요한지 본문을 인용해서 설명해 주세요.\"})\n",
    "    # answer = await astream_graph(agent, {\"messages\": \"본문에서 십일조와 관련된 성경구절을 찾아주세요.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  + Exception Group Traceback (most recent call last):\n",
      "  |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3665, in run_code\n",
      "  |     await eval(code_obj, self.user_global_ns, self.user_ns)\n",
      "  |   File \"/var/folders/rc/w7m_bf7x39bfb8wclzs_8xsr0000gn/T/ipykernel_44989/1848944460.py\", line 10, in <module>\n",
      "  |     async with MultiServerMCPClient(\n",
      "  |                ^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/langchain_mcp_adapters/client.py\", line 357, in __aenter__\n",
      "  |     await self.connect_to_server(server_name, **connection)\n",
      "  |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/langchain_mcp_adapters/client.py\", line 170, in connect_to_server\n",
      "  |     await self.connect_to_server_via_sse(\n",
      "  |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/langchain_mcp_adapters/client.py\", line 276, in connect_to_server_via_sse\n",
      "  |     sse_transport = await self.exit_stack.enter_async_context(\n",
      "  |                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 659, in enter_async_context\n",
      "  |     result = await _enter(cm)\n",
      "  |              ^^^^^^^^^^^^^^^^\n",
      "  |   File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 210, in __aenter__\n",
      "  |     return await anext(self.gen)\n",
      "  |            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/mcp/client/sse.py\", line 43, in sse_client\n",
      "  |     async with anyio.create_task_group() as tg:\n",
      "  |                ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 772, in __aexit__\n",
      "  |     raise BaseExceptionGroup(\n",
      "  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "  +-+---------------- 1 ----------------\n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n",
      "    |     yield\n",
      "    |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 394, in handle_async_request\n",
      "    |     resp = await self._pool.handle_async_request(req)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py\", line 256, in handle_async_request\n",
      "    |     raise exc from None\n",
      "    |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py\", line 236, in handle_async_request\n",
      "    |     response = await connection.handle_async_request(\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py\", line 101, in handle_async_request\n",
      "    |     raise exc\n",
      "    |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py\", line 78, in handle_async_request\n",
      "    |     stream = await self._connect(request)\n",
      "    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py\", line 124, in _connect\n",
      "    |     stream = await self._network_backend.connect_tcp(**kwargs)\n",
      "    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/httpcore/_backends/auto.py\", line 31, in connect_tcp\n",
      "    |     return await self._backend.connect_tcp(\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py\", line 113, in connect_tcp\n",
      "    |     with map_exceptions(exc_map):\n",
      "    |          ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 158, in __exit__\n",
      "    |     self.gen.throw(value)\n",
      "    |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n",
      "    |     raise to_exc(exc) from exc\n",
      "    | httpcore.ConnectError: All connection attempts failed\n",
      "    | \n",
      "    | The above exception was the direct cause of the following exception:\n",
      "    | \n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/mcp/client/sse.py\", line 47, in sse_client\n",
      "    |     async with aconnect_sse(\n",
      "    |                ^^^^^^^^^^^^^\n",
      "    |   File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 210, in __aenter__\n",
      "    |     return await anext(self.gen)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/httpx_sse/_api.py\", line 69, in aconnect_sse\n",
      "    |     async with client.stream(method, url, headers=headers, **kwargs) as response:\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 210, in __aenter__\n",
      "    |     return await anext(self.gen)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1583, in stream\n",
      "    |     response = await self.send(\n",
      "    |                ^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1629, in send\n",
      "    |     response = await self._send_handling_auth(\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1657, in _send_handling_auth\n",
      "    |     response = await self._send_handling_redirects(\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1694, in _send_handling_redirects\n",
      "    |     response = await self._send_single_request(request)\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1730, in _send_single_request\n",
      "    |     response = await transport.handle_async_request(request)\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 393, in handle_async_request\n",
      "    |     with map_httpcore_exceptions():\n",
      "    |          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 158, in __exit__\n",
      "    |     self.gen.throw(value)\n",
      "    |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n",
      "    |     raise mapped_exc(message) from exc\n",
      "    | httpx.ConnectError: All connection attempts failed\n",
      "    +------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from utils import ainvoke_graph, astream_graph\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model_name=\"gpt-4.1\", temperature=0, max_tokens=20000\n",
    ")\n",
    "\n",
    "async with MultiServerMCPClient(\n",
    "    {\n",
    "        \"weather\": {\n",
    "            # 서버의 포트와 일치해야 합니다.(8005번 포트)\n",
    "            \"url\": \"http://localhost:8005/sse\",\n",
    "            \"transport\": \"sse\",\n",
    "        }\n",
    "    }\n",
    ") as client:\n",
    "    print(client.get_tools())\n",
    "    agent = create_react_agent(model, client.get_tools())\n",
    "    answer = await astream_graph(agent, {\"messages\": \"서울의 현재 시간은?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음의 경우에는 session 이 닫혔기 때문에 도구에 접근할 수 없는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36magent\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "현재 저는 실시간 인터넷 접속이 불가능하여 최신 서울의 날씨 정보를 직접 제공할 수 없습니다. 하지만 일반적으로 6월의 서울은 초여름 날씨로, 평균 기온은 20~28도 사이이며, 습도가 높고 간혹 소나기가 내릴 수 있습니다.\n",
      "\n",
      "정확한 오늘의 날씨를 확인하려면 네이버, 다음, 기상청, 또는 스마트폰의 날씨 앱을 참고해 주세요! 필요하다면 서울의 계절별 기후 특징이나 여행 팁도 알려드릴 수 있습니다."
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node': 'agent',\n",
       " 'content': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_a1102cf978'}, id='run-40cb5132-6563-454d-83ec-cc4fb6567166'),\n",
       " 'metadata': {'langgraph_step': 1,\n",
       "  'langgraph_node': 'agent',\n",
       "  'langgraph_triggers': ('branch:to:agent',),\n",
       "  'langgraph_path': ('__pregel_pull', 'agent'),\n",
       "  'langgraph_checkpoint_ns': 'agent:b4bf4fc1-cd57-415d-f75a-adbb5b07bcb2',\n",
       "  'checkpoint_ns': 'agent:b4bf4fc1-cd57-415d-f75a-adbb5b07bcb2',\n",
       "  'ls_provider': 'openai',\n",
       "  'ls_model_name': 'gpt-4.1',\n",
       "  'ls_model_type': 'chat',\n",
       "  'ls_temperature': 0.0,\n",
       "  'ls_max_tokens': 20000}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await astream_graph(agent, {\"messages\": \"서울의 현재 시간은?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 그럼 Async Session 을 유지하며 도구에 접근하는 방식으로 변경해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StructuredTool(name='get_weather', description='\\n    Get current weather information for the specified location.\\n\\n    This function simulates a weather service by returning a fixed response.\\n    In a production environment, this would connect to a real weather API.\\n\\n    Args:\\n        location (str): The name of the location (city, region, etc.) to get weather for\\n\\n    Returns:\\n        str: A string containing the weather information for the specified location\\n    ', args_schema={'properties': {'location': {'title': 'Location', 'type': 'string'}}, 'required': ['location'], 'title': 'get_weatherArguments', 'type': 'object'}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x10b898fe0>)]\n"
     ]
    }
   ],
   "source": [
    "# 1. 클라이언트 생성\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"weather\": {\n",
    "            \"url\": \"http://localhost:8005/sse\",\n",
    "            \"transport\": \"sse\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# 2. 명시적으로 연결 초기화 (이 부분이 필요함)\n",
    "# 초기화\n",
    "await client.__aenter__()\n",
    "\n",
    "# 이제 도구가 로드됨\n",
    "print(client.get_tools())  # 도구가 표시됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "langgraph 의 에이전트를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에이전트 생성\n",
    "agent = create_react_agent(model, client.get_tools())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래프를 실행하여 결과를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36magent\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mtools\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "It's always Sunny in 서울\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36magent\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "서울의 날씨는 항상 맑음입니다! 오늘도 화창한 하루를 보내실 수 있겠네요."
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node': 'agent',\n",
       " 'content': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_cf18407276'}, id='run-3f2f12d4-9aef-4379-9d53-3a83e3b18046'),\n",
       " 'metadata': {'langgraph_step': 3,\n",
       "  'langgraph_node': 'agent',\n",
       "  'langgraph_triggers': ('branch:to:agent',),\n",
       "  'langgraph_path': ('__pregel_pull', 'agent'),\n",
       "  'langgraph_checkpoint_ns': 'agent:bfbec7ec-c929-5bdd-1650-5caa310cdd02',\n",
       "  'checkpoint_ns': 'agent:bfbec7ec-c929-5bdd-1650-5caa310cdd02',\n",
       "  'ls_provider': 'openai',\n",
       "  'ls_model_name': 'gpt-4.1',\n",
       "  'ls_model_type': 'chat',\n",
       "  'ls_temperature': 0.0,\n",
       "  'ls_max_tokens': 20000}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await astream_graph(agent, {\"messages\": \"서울의 현재 시간은?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stdio 통신 방식\n",
    "\n",
    "Stdio 통신 방식은 로컬 환경에서 사용하기 위해 사용합니다.\n",
    "\n",
    "- 통신을 위해 표준 입력/출력 사용\n",
    "\n",
    "참고: 아래의 python 경로는 수정하세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from utils import ainvoke_graph, astream_graph\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model_name=\"gpt-4.1\", temperature=0, max_tokens=20000\n",
    ")\n",
    "\n",
    "# StdIO 서버 파라미터 설정\n",
    "# - command: Python 인터프리터 경로\n",
    "# - args: 실행할 MCP 서버 스크립트\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"./.venv/bin/python\",\n",
    "    # args=[\"mcp_server_local.py\"],\n",
    "    args=[\"./resources/mcp_rag_help_center/mcp_server.py\"],\n",
    ")\n",
    "\n",
    "# StdIO 클라이언트를 사용하여 서버와 통신\n",
    "async with stdio_client(server_params) as (read, write):\n",
    "    # 클라이언트 세션 생성\n",
    "    async with ClientSession(read, write) as session:\n",
    "        # 연결 초기화\n",
    "        await session.initialize()\n",
    "\n",
    "        # MCP 도구 로드\n",
    "        tools = await load_mcp_tools(session)\n",
    "        print(tools)\n",
    "\n",
    "        # 에이전트 생성\n",
    "        agent = create_react_agent(model, tools)\n",
    "\n",
    "        # 에이전트 응답 스트리밍\n",
    "        # await astream_graph(agent, {\"messages\": \"서울의 날씨는 어떠니?\"})\n",
    "        await astream_graph(agent, {\"messages\": \"창작자에 대해 설명해줘\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG 를 구축한 MCP 서버 사용\n",
    "\n",
    "- 파일: `mcp_server_rag.py`\n",
    "\n",
    "사전에 langchain 으로 구축한 `mcp_server_rag.py` 파일을 사용합니다.\n",
    "\n",
    "stdio 통신 방식으로 도구에 대한 정보를 가져옵니다. 여기서 도구는 `retriever` 도구를 가져오게 되며, 이 도구는 `mcp_server_rag.py` 에서 정의된 도구입니다. 이 파일은 사전에 서버에서 실행되지 **않아도** 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36magent\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mtools\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "정책･법제\n",
      "기업･산업\n",
      "기술･연구\n",
      "인력･교육\n",
      "3\n",
      "파리 AI 행동 정상회의, AI의 공공성·지속가능성과 함께 규제 완화 논의\n",
      "n 파리 AI 행동 정상회의의 주요 결과물로 포용적이고 지속가능한 AI를 위한 선언문이 발표되었으며, \n",
      "공익을 위한 AI 프로젝트 ‘커런트 AI’도 출범\n",
      "n 파리 AI 정상회의에서는 AI 규제 완화와 투자 확대가 핵심 의제로 떠올랐으며, 마크롱 프랑스 \n",
      "대통령은 1,090억 유로 규모의 AI 인프라 민간 투자 프로젝트도 발표\n",
      "KEY Contents\n",
      "£ 미국과 영국을 제외한 60개국, 포용적이고 지속가능한 AI에 대한 선언문 발표\n",
      "n 프랑스 정부가 2025년 2월 10~11일, 파리에서 ‘AI 행동 정상회의(AI Action Summit)’를 개최\n",
      "∙이번 정상회의에는 전 세계 87개 국가에서 기업, 국제기구, 시민단체 등 총 1,000여 명이 참여해 AI 글로벌 \n",
      "거버넌스를 심도 있게 논의\n",
      "n 정상회의 주요 결과물로 한국을 포함한 60개국이 공동 참여한 ‘인류와 지구를 위한 포용적이고 \n",
      "지속가능한 AI에 대한 선언문’이 발표되었고 미국과 영국은 선언에 불참    \n",
      "∙선언문은 공익을 위한 AI 및 지속가능한 AI를 목표로 제시하고, 이를 달성하기 위한 구체적 행동으로 \n",
      "①공익을 위한 AI 플랫폼 및 인큐베이터 출범 ②환경적 지속가능성을 위한 AI 에너지 관측소 설립 \n",
      "③일자리에 대한 AI 영향 관측 네트워크를 제시\n",
      "n 이번 정상회의에서는 9개국*과 구글, 세일즈포스 등의 기업이 4억 달러를 투자해 공익을 위한 \n",
      "AI 프로젝트 ‘커런트 AI(Current AI)’도 출범\n",
      "* 나이지리아, 독일, 모로코, 스위스, 슬로베니아, 칠레, 케냐, 프랑스, 핀란드  \n",
      "∙이 프로젝트는 공익 AI 환경 조성을 목표로 △AI 훈련을 위한 고품질 공공 데이터 접근성 확대 \n",
      "△오픈소스 인프라 지원 △AI의 사회적·환경적 영향을 측정하기 위한 시스템 개발 지원을 추진\n",
      "£ 파리 AI 정상회의, AI 규제 완화 및 AI 투자 확대가 핵심 화두로 부상\n",
      "SPRi AI Brief\n",
      "2025년 3월호\n",
      "4\n",
      "EU 집행위원회, 경쟁력 강화 로드맵의 일환으로 AI 기가팩토리 구축 추진\n",
      "n EU 집행위원회가 5개년 정책 로드맵 ‘경쟁력 나침반’에 따라 혁신 격차 해소를 위한 AI 정책으로 \n",
      "‘AI 기가팩토리’와 ‘AI 적용’ 전략을 제시\n",
      "n EU 집행위원회는 파리 AI 정상회의에서 총 2천억 유로 규모의 ‘인베스트AI’ 계획의 일환으로 \n",
      "200억 유로를 투입해 4개의 AI 기가팩토리를 건설하겠다고 발표  \n",
      "KEY Contents\n",
      "£ EU 집행위원회, 5개년 정책 로드맵 하에서 AI 기가팩토리와 AI 적용 전략 추진 \n",
      "n 우르줄라 폰 데어 라이엔(Ursula von der Leyen) EU 집행위원장이 2025년 1월 29일 2기 EU \n",
      "집행부(2024년 12월 출범)의 5개년 정책 로드맵 ‘경쟁력 나침반(Competitive Compass)’을 발표\n",
      "∙EU 집행위원회는 경쟁력 제고를 위해 △혁신 격차 해소 △脫탄소화 △공급망 안보의 3개 영역을 중점 \n",
      "과제로 제시했으며, 이중 혁신 격차 해소와 관련해 AI 정책을 포함\n",
      "n EU 집행위원회는 핵심 분야의 AI 개발과 산업계 AI 도입 활성화를 위한 ‘AI 기가팩토리(AI Gigafactory)’와 \n",
      "‘AI 적용(Apply AI)’ 전략을 제안\n",
      "∙AI 기가팩토리는 입법 추진 예정인 ‘EU 클라우드 및 AI 개발법(EU Cloud and AI Development \n",
      "Act)’*을 통해 공공과 민간 자금을 활용하여 초거대 AI 모델 훈련에 특화된 대규모 데이터센터를 \n",
      "구축함으로써 EU 전역의 AI 생태계를 활성화한다는 계획\n",
      "* 고성능 연산 자원과 디지털 인프라에 대한 강력한 규제 프레임워크를 통해 클라우드와 AI 분야에서 유럽의 리더십 강화를 위한 법안\n",
      "∙AI 적용 전략은 제조업, 에너지, 자동차, 로봇공학, 제약, 항공, 금융 서비스 산업의 AI 적용을 촉진하고 \n",
      "보건과 사법 등의 공공서비스 개선을 추진\n",
      "CONTENTS\n",
      "정책･법제\n",
      "∙미국 국가AI자문위원회, 트럼프 행정부 대상 AI 정책 보고서 발간\n",
      "2\n",
      "∙파리 AI 행동 정상회의, AI의 공공성·지속가능성과 함께 규제 완화 논의\n",
      "3\n",
      "∙EU 집행위원회, 경쟁력 강화 로드맵의 일환으로 AI 기가팩토리 구축 추진\n",
      "4\n",
      "∙EU 집행위원회, 2025년 업무 프로그램에서 ‘AI 책임 지침’ 철회 계획 발표\n",
      "5\n",
      "∙영국, AI안전연구소의 명칭 AI보안연구소로 변경하고 앤스로픽과 AI 협력 발표\n",
      "6\n",
      "기업･산업\n",
      "∙구글, 성능 개선한 ‘제미나이 2.0’ 제품군 확대 출시\n",
      "8\n",
      "∙ 오픈AI, 심층 조사를 수행하는 에이전트 ‘딥 리서치’ 공개\n",
      "9\n",
      "∙오픈AI, GPT-5 출시 로드맵 발표 및 GPT-4.5 프리뷰 공개\n",
      "10\n",
      "∙xAI, 최신 AI 모델 ‘그록 3’ 프리뷰 출시\n",
      "11\n",
      "∙퍼플렉시티, 심층 조사와 분석을 수행하는 ‘딥 리서치’ 무료 출시\n",
      "12\n",
      "∙알리바바, 딥시크 V3 능가하는 성능의 ‘큐원2.5-Max’ 출시\n",
      "13\n",
      "∙아크 인베스트, 2025년 혁신 기술 중 하나로 AI 에이전트 선정\n",
      "14\n",
      "기술･연구\n",
      "∙상하이교통⼤ 연구진, 소량의 고품질 데이터를 활용한 추론 AI 모델 개발\n",
      "16\n",
      "∙스탠포드⼤와 워싱턴⼤ 연구진, 저비용으로 고성능 추론 AI 모델 개발\n",
      "17\n",
      "∙바이트댄스, 인물 움직임을 생성하는 AI 모델 ‘옴니휴먼-1’ 개발\n",
      "18\n",
      "∙AI안전센터와 스케일 AI, 고난도 벤치마크 ‘HLE’ 공개\n",
      "19\n",
      "인력･교육 \n",
      "∙딜로이트 조사 결과, 전 세계 기업들은 점진적으로 AI 도입 확대\n",
      "21\n",
      "∙마이크로소프트 연구 결과, 생성 AI 신뢰할수록 비판적 사고 감소\n",
      "22\n",
      "∙앤스로픽, AI가 노동시장에 미치는 영향을 분석한 ‘앤스로픽 경제 지수’ 공개 \n",
      "23\n",
      "∙앤스로픽을 비롯한 여러 기업들, 입사 지원 시 AI 도구 사용금지 요구\n",
      "24\n",
      "주요행사일정\n",
      "25\n",
      "| 2025년 3월호 |\n",
      "SPRi AI Brief\n",
      "2025년 3월호\n",
      "2\n",
      "미국 국가AI자문위원회, 트럼프 행정부 대상 AI 정책 보고서 발간\n",
      "n AI 정책에 관하여 대통령과 연방 정부에 자문을 제공하는 미국 국가AI자문위원회(NAIAC)가 \n",
      "트럼프 행정부가 우선적으로 추진해야 할 AI 정책을 정리한 보고서를 발표\n",
      "n 보고서는 정책 우선순위로 △노동력 △AI 인식과 리터러시 △교육 △과학 △보건 △정부 \n",
      "△중소기업 지원 △AI 거버넌스 △미국 시민 △법 집행을 제시\n",
      "KEY Contents\n",
      "£ 미국 국가AI자문위원회, 미국의 AI 주도권 유지를 위한 10대 정책 우선순위 제시\n",
      "n 미국 국가AI자문위원회는 2025년 1월 28일 미국의 기술 주도권 유지를 위해 트럼프 행정부가 추진\n",
      "해야 할 AI 중점 분야를 제시한 보고서를 발표하고, 정책 우선순위로 △노동력 △AI 인식과 리터러시 \n",
      "△교육 △과학 △보건 △정부 △중소기업 지원 △AI 거버넌스 △미국 시민 △법 집행을 선정\n",
      "∙(노동력) AI가 노동시장에 미치는 영향에 대응해 연방 정부와 주·지방 정부 간 협력을 강화하고 AI를 비롯한 \n",
      "기술 개발 지원 등 AI로 실직 위험에 처한 근로자 지원 전략을 마련\n",
      "∙(AI 인식과 리터러시) AI의 광범위한 도입을 위해 전국 규모의 AI 인식 제고 캠페인을 시행하고 AI 기초 \n",
      "교육, 전문과정, AI 자격증 과정과 같은 교육 프로그램을 강화\n",
      "∙(교육) 교육 환경에 특화된 AI 위험관리 프레임워크를 개발하고 AI 교육 과정과 AI 도구 개발 해커톤을 개최\n",
      "∙(과학) 중요한 사회 문제 해결을 위해 AI 적용을 확대하고, 과학 분야의 AI 연구에 대한 자금 우선순위를 \n",
      "설정하며 지원을 강화하는 전략을 추진\n",
      "∙(보건) 백악관 과학기술위원회(NSTC) 산하 AI특별위원회 내에 개인 건강정보의 안전하고 책임 있는 \n",
      "사용을 담당할 소위원회를 신설 \n",
      "∙(정부) AI로 정부의 운영 효율성을 향상하기 위해 국가AI이니셔티브 사무국에 인력과 자원을 충원하고 AI \n",
      "모델 평가 프레임워크를 구축\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36magent\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "파리 AI 행동 정상회의(AI Action Summit)는 2025년 2월 10~11일 프랑스 파리에서 개최된 글로벌 AI 정상회의입니다. 이 회의에는 전 세계 87개국에서 정부, 기업, 국제기구, 시민단체 등 약 1,000여 명이 참여해 AI의 글로벌 거버넌스, 공공성, 지속가능성, 규제 완화, 투자 확대 등을 논의했습니다.\n",
      "\n",
      "주요 내용과 결과는 다음과 같습니다:\n",
      "\n",
      "1. **포용적이고 지속가능한 AI 선언문 발표**\n",
      "   - 한국을 포함한 60개국이 ‘인류와 지구를 위한 포용적이고 지속가능한 AI에 대한 선언문’을 공동 발표했습니다(미국, 영국은 불참).\n",
      "   - 선언문은 공익을 위한 AI, 지속가능한 AI를 목표로 하며, 구체적 행동으로\n",
      "     1) 공익을 위한 AI 플랫폼 및 인큐베이터 출범,\n",
      "     2) 환경적 지속가능성을 위한 AI 에너지 관측소 설립,\n",
      "     3) 일자리에 대한 AI 영향 관측 네트워크 구축 등을 제시했습니다.\n",
      "\n",
      "2. **공익 AI 프로젝트 ‘커런트 AI(Current AI)’ 출범**\n",
      "   - 9개국(프랑스, 독일, 핀란드, 스위스, 슬로베니아, 케냐, 나이지리아, 모로코, 칠레)과 구글, 세일즈포스 등 글로벌 기업이 4억 달러를 투자해 공익 AI 프로젝트를 시작했습니다.\n",
      "   - 이 프로젝트는 고품질 공공 데이터 접근성 확대, 오픈소스 인프라 지원, AI의 사회적·환경적 영향 측정 시스템 개발 등을 목표로 합니다.\n",
      "\n",
      "3. **AI 규제 완화 및 투자 확대 논의**\n",
      "   - 마크롱 프랑스 대통령은 1,090억 유로 규모의 AI 인프라 민간 투자 프로젝트를 발표했습니다.\n",
      "   - AI 규제 완화와 투자 확대가 핵심 의제로 부상했습니다.\n",
      "\n",
      "4. **EU의 AI 정책 발표**\n",
      "   - EU 집행위원회는 2,000억 유로 규모의 ‘인베스트AI’ 계획의 일환으로 200억 유로를 투입해 4개의 AI 기가팩토리(대규모 AI 데이터센터)를 건설하겠다고 발표했습니다.\n",
      "   - AI 적용 전략을 통해 제조업, 에너지, 자동차, 로봇공학, 제약, 항공, 금융 서비스 등 다양한 산업에 AI 도입을 촉진할 계획입니다.\n",
      "\n",
      "**요약**: 파리 AI 행동 정상회의는 AI의 공공성, 지속가능성, 규제 완화, 투자 확대를 논의하는 글로벌 협력의 장으로, 포용적 AI 선언문과 공익 AI 프로젝트 출범 등 실질적 결과를 도출한 것이 특징입니다."
     ]
    }
   ],
   "source": [
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from utils import astream_graph\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model_name=\"gpt-4.1\", temperature=0, max_tokens=20000\n",
    ")\n",
    "\n",
    "# RAG 서버를 위한 StdIO 서버 파라미터 설정\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"./.venv/bin/python\",\n",
    "    args=[\"./mcp_server_rag.py\"],\n",
    ")\n",
    "\n",
    "# StdIO 클라이언트를 사용하여 RAG 서버와 통신\n",
    "async with stdio_client(server_params) as (read, write):\n",
    "    # 클라이언트 세션 생성\n",
    "    async with ClientSession(read, write) as session:\n",
    "        # 연결 초기화\n",
    "        await session.initialize()\n",
    "\n",
    "        # MCP 도구 로드 (여기서는 retriever 도구)\n",
    "        tools = await load_mcp_tools(session)\n",
    "\n",
    "        # 에이전트 생성 및 실행\n",
    "        agent = create_react_agent(model, tools)\n",
    "\n",
    "        # 에이전트 응답 스트리밍\n",
    "        await astream_graph(\n",
    "            agent, {\"messages\": \"파리 AI 행동 정상회의에 대해 설명해줘\"}\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSE 방식과 StdIO 방식 혼합 사용\n",
    "\n",
    "- 파일: `mcp_server_rag.py` 는 StdIO 방식으로 통신\n",
    "- `langchain-dev-docs` 는 SSE 방식으로 통신\n",
    "\n",
    "SSE 방식과 StdIO 방식을 혼합하여 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model_name=\"gpt-4.1\", temperature=0, max_tokens=20000\n",
    ")\n",
    "\n",
    "# 1. 다중 서버 MCP 클라이언트 생성\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"document-retriever\": {\n",
    "            \"command\": \"./.venv/bin/python\",\n",
    "            # mcp_server_rag.py 파일의 절대 경로로 업데이트해야 합니다\n",
    "            \"args\": [\"./mcp_server_rag.py\"],\n",
    "            # stdio 방식으로 통신 (표준 입출력 사용)\n",
    "            \"transport\": \"stdio\",\n",
    "        },\n",
    "        \"langchain-dev-docs\": {\n",
    "            # SSE 서버가 실행 중인지 확인하세요\n",
    "            \"url\": \"https://teddynote.io/mcp/langchain/sse\",\n",
    "            # SSE(Server-Sent Events) 방식으로 통신\n",
    "            \"transport\": \"sse\",\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# 2. 비동기 컨텍스트 매니저를 통한 명시적 연결 초기화\n",
    "await client.__aenter__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "langgraph 의 `create_react_agent` 를 사용하여 에이전트를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "prompt = (\n",
    "    \"You are a smart agent. \"\n",
    "    \"Use `retriever` tool to search on AI related documents and answer questions.\"\n",
    "    \"Use `langchain-dev-docs` tool to search on langchain / langgraph related documents and answer questions.\"\n",
    "    \"Answer in Korean.\"\n",
    ")\n",
    "agent = create_react_agent(\n",
    "    model, client.get_tools(), prompt=prompt, checkpointer=MemorySaver()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구축해 놓은 `mcp_server_rag.py` 에서 정의한 `retriever` 도구를 사용하여 검색을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RunnableConfig(recursion_limit=30, thread_id=1)\n",
    "await astream_graph(\n",
    "    agent,\n",
    "    {\n",
    "        \"messages\": \"`retriever` 도구를 사용해서 파리 AI 행동 정상회의에 대해 설명해줘\"\n",
    "    },\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 `langchain-dev-docs` 도구를 사용하여 검색을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RunnableConfig(recursion_limit=30, thread_id=1)\n",
    "await astream_graph(\n",
    "    agent,\n",
    "    {\"messages\": \"langgraph-dev-docs 참고해서 self-rag 의 정의에 대해서 알려줘\"},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MemorySaver` 를 사용하여 단기 기억을 유지합니다. 따라서, multi-turn 대화도 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await astream_graph(\n",
    "    agent, {\"messages\": \"이전의 내용을 bullet point 로 요약해줘\"}, config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain 에 통합된 도구 + MCP 도구\n",
    "\n",
    "여기서는 LangChain 에 통합된 도구를 기존의 MCP 로만 이루어진 도구와 함께 사용이 가능한지 테스트 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# Tavily 검색 도구를 초기화 합니다. (news 타입, 최근 3일 내 뉴스)\n",
    "tavily = TavilySearchResults(max_results=3, topic=\"news\", days=3)\n",
    "\n",
    "# 기존의 MCP 도구와 함께 사용합니다.\n",
    "tools = client.get_tools() + [tavily]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "langgraph 의 `create_react_agent` 를 사용하여 에이전트를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "# 재귀 제한 및 스레드 아이디 설정\n",
    "config = RunnableConfig(recursion_limit=30, thread_id=2)\n",
    "\n",
    "# 프롬프트 설정\n",
    "prompt = \"You are a smart agent with various tools. Answer questions in Korean.\"\n",
    "\n",
    "# 에이전트 생성\n",
    "agent = create_react_agent(model, tools, prompt=prompt, checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "새롭게 추가한 `tavily` 도구를 사용하여 검색을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await astream_graph(agent, {\"messages\": \"오늘 뉴스 찾아줘\"}, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`retriever` 도구가 원활하게 작동하는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await astream_graph(\n",
    "    agent,\n",
    "    {\n",
    "        \"messages\": \"`retriever` 도구를 사용해서 삼성전자가 개발한 생성형 AI 이름을 검색해줘\"\n",
    "    },\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smithery 에서 제공하는 MCP 서버\n",
    "\n",
    "- 링크: https://smithery.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사용한 도구 목록은 아래와 같습니다.\n",
    "\n",
    "- Sequential Thinking: https://smithery.ai/server/@smithery-ai/server-sequential-thinking\n",
    "  - 구조화된 사고 프로세스를 통해 역동적이고 성찰적인 문제 해결을 위한 도구를 제공하는 MCP 서버\n",
    "- Desktop Commander: https://smithery.ai/server/@wonderwhy-er/desktop-commander\n",
    "  - 다양한 편집 기능으로 터미널 명령을 실행하고 파일을 관리하세요. 코딩, 셸 및 터미널, 작업 자동화\n",
    "\n",
    "**참고**\n",
    "\n",
    "- smithery 에서 제공하는 도구를 JSON 형식으로 가져올때, 아래의 예시처럼 `\"transport\": \"stdio\"` 로 꼭 설정해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# LLM 모델 초기화\n",
    "model = ChatAnthropic(model=\"claude-3-7-sonnet-latest\", temperature=0, max_tokens=20000)\n",
    "\n",
    "# 1. 클라이언트 생성\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"server-sequential-thinking\": {\n",
    "            \"command\": \"npx\",\n",
    "            \"args\": [\n",
    "                \"-y\",\n",
    "                \"@smithery/cli@latest\",\n",
    "                \"run\",\n",
    "                \"@smithery-ai/server-sequential-thinking\",\n",
    "                \"--key\",\n",
    "                \"89a4780a-53b7-4b7b-92e9-a29815f2669b\",\n",
    "            ],\n",
    "            \"transport\": \"stdio\",  # stdio 방식으로 통신을 추가합니다.\n",
    "        },\n",
    "        \"desktop-commander\": {\n",
    "            \"command\": \"npx\",\n",
    "            \"args\": [\n",
    "                \"-y\",\n",
    "                \"@smithery/cli@latest\",\n",
    "                \"run\",\n",
    "                \"@wonderwhy-er/desktop-commander\",\n",
    "                \"--key\",\n",
    "                \"89a4780a-53b7-4b7b-92e9-a29815f2669b\",\n",
    "            ],\n",
    "            \"transport\": \"stdio\",  # stdio 방식으로 통신을 추가합니다.\n",
    "        },\n",
    "        \"document-retriever\": {\n",
    "            \"command\": \"./.venv/bin/python\",\n",
    "            # mcp_server_rag.py 파일의 절대 경로로 업데이트해야 합니다\n",
    "            \"args\": [\"./mcp_server_rag.py\"],\n",
    "            # stdio 방식으로 통신 (표준 입출력 사용)\n",
    "            \"transport\": \"stdio\",\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# 2. 명시적으로 연결 초기화\n",
    "await client.__aenter__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "langgraph 의 `create_react_agent` 를 사용하여 에이전트를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "config = RunnableConfig(recursion_limit=30, thread_id=3)\n",
    "agent = create_react_agent(model, client.get_tools(), checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Desktop Commander` 도구를 사용하여 터미널 명령을 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await astream_graph(\n",
    "    agent,\n",
    "    {\n",
    "        \"messages\": \"현재 경로를 포함한 하위 폴더 구조를 tree 로 그려줘. 단, .venv 폴더는 제외하고 출력해줘.\"\n",
    "    },\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 `Sequential Thinking` 도구를 사용하여 비교적 복잡한 작업을 수행할 수 있는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await astream_graph(\n",
    "    agent,\n",
    "    {\n",
    "        \"messages\": (\n",
    "            \"`retriever` 도구를 사용해서 삼성전자가 개발한 생성형 AI 관련 내용을 검색하고 \"\n",
    "            \"`Sequential Thinking` 도구를 사용해서 보고서를 작성해줘.\"\n",
    "        )\n",
    "    },\n",
    "    config=config,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
