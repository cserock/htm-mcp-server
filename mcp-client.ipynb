{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCP + LangGraph Client Example\n",
    "\n",
    "**참고자료**\n",
    "- https://modelcontextprotocol.io/introduction\n",
    "- https://github.com/langchain-ai/langchain-mcp-adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경설정\n",
    "\n",
    "아래 설치 방법을 참고하여 `uv` 를 설치합니다.\n",
    "\n",
    "**uv 설치 방법**\n",
    "\n",
    "```bash\n",
    "# macOS/Linux\n",
    "curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# Windows (PowerShell)\n",
    "irm https://astral.sh/uv/install.ps1 | iex\n",
    "```\n",
    "\n",
    "**의존성 설치**\n",
    "\n",
    "```bash\n",
    "uv pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "환경변수를 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiServerMCPClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사전에 `mcp_server_remote.py` 를 실행해둡니다. 터미널을 열고 가상환경이 활성화 되어 있는 상태에서 서버를 실행해 주세요.\n",
    "\n",
    "> 명령어\n",
    "```bash\n",
    "source .venv/bin/activate\n",
    "python mcp_server_remote.py\n",
    "```\n",
    "\n",
    "`async with` 로 일시적인 Session 연결을 생성 후 해제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StructuredTool(name='search', description='\\n    Performs hybrid search (keyword + semantic) on MD documents.\\n    Combines exact keyword matching and semantic similarity to deliver optimal results.\\n    The most versatile search option for general questions or when unsure which search type is best.\\n    \\n    Parameters:\\n        query: Search query\\n        top_k: Number of results to return\\n\\n    ', args_schema={'properties': {'query': {'title': 'Query', 'type': 'string'}, 'top_k': {'default': 4, 'title': 'Top K', 'type': 'integer'}}, 'required': ['query'], 'title': 'searchArguments', 'type': 'object'}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x10f15ac00>)]\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36magent\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mtools\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "## Search Results\n",
      "\n",
      "### Result 1 (Page: 226)\n",
      "\n",
      "하나님은 당신의 물질이 필요한 것이 아니다. 하나님께서는 당신의 삶이 풍성해지기를 원하고 더 축복해\n",
      "주시기를 원하신다. 그 하나님의 은혜를 누리기 위해서는 물질보다 하나님을 신뢰하는 것을 배워야 한\n",
      "다. 그리고 일에 대한 개념이 바뀌어야 한다.\n",
      "# 2) 십일조\n",
      "▶ 십일조를 통하여 당신 믿음을 실제적으로 증명하라.\n",
      "하나님은 우리의 돈을 필요로 하시지 않는다. 당신의 소유권, 통치권, 공급권을 이양하는 가장 실제적\n",
      "인 일이 무엇인가? 그것은 바로 당신이 벌었다고 생각하는 물질의 일부를 하나님께 돌려드리는 것이다.\n",
      "고전 4:7 누가 너를 남달리 구별하였느냐 네게 있는 것 중에 받지 아니한 것이 무엇이냐 네가 받았은즉 어찌하여 받\n",
      "지 아니한 것 같이 자랑하느냐\n",
      "▶ 십일조를 통하여 물질의 대속함을 받아라.\n",
      "생각해 보라. 당신의 번 돈을 100% 당신이 관리하는 것보다 소유의 10%를 주님께 드림으로써 나머지\n",
      "90%를 하나님께서 관리해 주시는 것이 훨씬 좋다. 사람들은 10%를 아까워한다. 그래서 계산할 때 반올\n",
      "림하지 않는다. 당신이 번 돈 10%를 하나님께 드린다고 생각한다.\n",
      "그리고 그 10%가 어디에 어떻게 사용될까에 너무나도 관심이 많다. 그러나 모르고 있는 사실은 당신이\n",
      "가지고 있는 90%의 돈에 대한 것이다. 그 돈을 당신이 소유할 때는 하나님의 보호하심을 받지 못한다.\n",
      "다른 말로 당신의 관리하고 있는 90% 돈의 축복을 위해 10%가 필요한 것이지, 당신 소유의 10%를 당신\n",
      "이 포기한다는 의미가 아니다.\n",
      "출 13:2 이스라엘 자손 중에 사람이나 짐승이나 무론하고 초태생은 다 거룩히 구별하여 내게 돌리라 이는 내 것이\n",
      "니라 하시니라\n",
      "레 27:30 땅의 십분 일 곧 땅의 곡식이나 나무의 과실이나 그 십분 일은 여호와의 것이니 여호와께 성물이라\n",
      "잠 3:9-10 (9) 네 재물과 네 소산물의 처음 익은 열매로 여호와를 공경하라 (10) 그리하면 네 창고가 가득히 차고\n",
      "네 즙틀에 새 포도즙이 넘치리라\n",
      "# ▶ 십일조는 반드시 해야 하는가?\n",
      "\n",
      "Source: 4a414ed1-67ae-49f6-acf5-a92ad7a3c206_31_kbs_cleaned.pdf\n",
      "\n",
      "---\n",
      "\n",
      "### Result 2 (Page: 228)\n",
      "\n",
      "십일조에 대한 질문들 중에 빈번하게 등장하는 질문은 “한 곳에 십일조 전부를 해야 하는가 아니면 여\n",
      "러 군데 나눠서 내어도 되는가에 대한 것”이다.\n",
      "우리는 한 곳에 십일조 전부를 드리지 않으면, 1) 교회에서 내 재정이 시원치 않구나 혹은 2) 십일조 생활\n",
      "을 제대로 하지 않구나라고 생각할까 염려하기도 한다. 그것이 마음에 걸린다면 그곳에 전부를 내는 편\n",
      "을 택하라. 그러나 하나님의 인도하심으로 마음에 평안이 온다면 당신의 영적 양식을 공급하는 곳에 나\n",
      "누어 내어도 괜찮다.\n",
      "십일조는 의무감으로 하는 것이 아니다. 구약에 있어 십일조는 지켜야 할 규례이다. 그러나 새 언약 하에\n",
      "서 십일조는 하나님의 자녀로서 모든 재물이 주님의 것임을 고백하는 믿음의 행위이다. 당신에게 도와달\n",
      "라고 사정하거나, 부담을 주는 곳이거나, 축복받을 것이라고 약속하거나, 항상 십일조를 해 왔던 곳이라는 이유\n",
      "\n",
      "Source: 4a414ed1-67ae-49f6-acf5-a92ad7a3c206_31_kbs_cleaned.pdf\n",
      "\n",
      "---\n",
      "\n",
      "### Result 3 (Page: 227)\n",
      "\n",
      "은 하나님을 기쁘시게 하도록 하기 위해서 십일조와 헌금하는 것이 아니라, 주의 자녀로서 오직 주를 신뢰\n",
      "하며 주께 감사함과 즐거움으로 주의 뜻을 더 나타내기 위해서 십일조와 헌금을 하는 것이다.\n",
      "말 3:8-10 (8) 사람이 어찌 하나님의 것을 도둑질하겠느냐 그러나 너희는 나의 것을 도둑질하고도 말하기를 우리\n",
      "가 어떻게 주의 것을 도둑질하였나이까 하는도다 이는 곧 십일조와 봉헌물이라 (9) 너희 곧 온 나라가 나의 것을 도\n",
      "둑질하였으므로 너희가 저주를 받았느니라 (10) 만군의 여호와가 이르노라 너희의 온전한 십일조를 창고에 들여 나\n",
      "의 집에 양식이 있게 하고 그것으로 나를 시험하여 내가 하늘 문을 열고 너희에게 복을 쌓을 곳이 없도록 붓지 아니\n",
      "하나 보라\n",
      "▶ 십일조는 율법 아래 있을 때만 행하는 것이 아닌가?\n",
      "많은 경우 우리는 은혜 아래 있기 때문에 율법을 지킬 필요가 없다고 생각한다. 그러나 생각해 보라. 아\n",
      "브라함이 멜기세덱(예수님을 예표)에게 십일조를 드린 것은 율법이 만들어지기 430년 전의 일이다. 또\n",
      "한, 십분의 일은 누가 받는 것인가? 영원히 살아계신 예수님이시다. 십일조는 율법이 주어지기도 전에\n",
      "있던 성경적 원리였다.\n",
      "창 14:18-20 (18) 살렘 왕 멜기세덱이 떡과 포도주를 가지고 나왔으니 그는 지극히 높으신 하나님의 제사장이었\n",
      "더라 (19) 그가 아브람에게 축복하여 가로되 천지의 주재시요 지극히 높으신 하나님이여 아브람에게 복을 주옵소서\n",
      "(20) 너희 대적을 네 손에 붙이신 지극히 높으신 하나님을 찬송할지로다 하매 아브람이 그 얻은 것에서 십분 일을 멜\n",
      "기세덱에게 주었더라\n",
      "히 7:1-10 (1) 이 멜기세덱은 살렘 왕이요 지극히 높으신 하나님의 제사장이라 여러 임금을 쳐서 죽이고 돌아오는\n",
      "아브라함을 만나 복을 빈 자라 (2) 아브라함이 일체 십분의 일을 그에게 나눠 주니라 그 이름을 번역한 즉 첫째 의의\n",
      "왕이요 또 살렘 왕이니 곧 평강의 왕이요 (3) 아비도 없고 어미도 없고 족보도 없고 시작한 날도 없고 생명의 끝도 없\n",
      "\n",
      "Source: 4a414ed1-67ae-49f6-acf5-a92ad7a3c206_31_kbs_cleaned.pdf\n",
      "\n",
      "---\n",
      "\n",
      "### Result 4 (Page: 109)\n",
      "\n",
      "우리는 예수 그리스도를 믿음으로 거듭났으며, 그리스도 안에서 새로운 피조물이 되었다. 당신은 이 말의 진정\n",
      "한 의미를 제대로 알고 있는가?\n",
      "구원은 오직 예수 그리스도를 믿음으로써 이루어진다.\n",
      "하나님나라의 삶은 진정한 회개를 통해서 자신이 누구인지를 알아야 살 수 있다.\n",
      "요 3:6 육으로 난 것은 육이요 영으로 난 것은 영이니\n",
      "우리가 거듭났다는 것은 이제 하나님의 영으로 태어났다는 것을 의미한다.\n",
      "요 1:13 이는 혈통으로나 육정으로나 사람의 뜻으로 나지 아니하고 오직 하나님께로부터 난 자들이니라\n",
      "# 다음 구절의 의미를 깊이 묵상해보라.\n",
      "롬 6:4 그러므로 우리가 그의 죽으심과 합하여 세례를 받음으로 그와 함께 장사되었나니 이는 아버지의 영광으로 말\n",
      "미암아 그리스도를 죽은 자 가운데서 살리심과 같이 우리로 또한 새 생명 가운데서 행하게 하려 함이라』\n",
      "“우리가 그의 죽으심과 합하여 세례를 받음으로”\n",
      "종종 이 구절의 의미를 물세례의 결과로 우리는 죄에 대하여 죽고 하나님에 대하여 사는 것이라고 생각\n",
      "하는 사람이 있다. 그러나 세례는 의식일 뿐이지 본질이 아니다. 물세례라는 그 의식 자체가 우리의 죄를\n",
      "사해 주는 것이 아니다. 세례의 본질적 의미는 예수 그리스도와의 연합과 씻음에 있다.\n",
      "연합의 의미는 예수 그리스도께서 나의 구원자이시며, 내 삶의 주님이시며, 살아계신 하나님의 아들이심\n",
      "을 믿고 받아들이는 것이다. 이때 “합하여”라는 의미가 바로 ‘into (헬, 에이스)’라는 전치사를 나\n",
      "타낸다. 즉, 내가 그리스도 안으로 들어가 연합하는 의미이다.\n",
      "● 예수 그리스도를 구원자와 주님으로 믿는다는 것은 바로 자신이 자신의 삶의 주인이라는 자기의식을 포기하\n",
      "는 것을 의미한다.\n",
      "● 즉 “더 이상 세상 신이 준 옛 본질에 내가 묶여 살지 않겠습니다, 나는 더 이상 마귀에게 속한 자가 아\n",
      "닙 니다.” 라고 고백하는 것이다.\n",
      "● “살아계신 예수 그리스도가 내 삶을 구원한 분이시며, 내 삶의 영원한 주인이십니다”라는 사실을 믿\n",
      "\n",
      "Source: 4a414ed1-67ae-49f6-acf5-a92ad7a3c206_31_kbs_cleaned.pdf\n",
      "\n",
      "---\n",
      "\n",
      "### Result 5 (Page: 228)\n",
      "\n",
      "우리는 십일조를 통하여 다음과 같은 것을 체험할 수 있다.\n",
      "- 십일조를 통해서 마음에 물질보다 하나님을 최우선 순위에 놓을 수 있다.\n",
      "- 십일조를 통해서 소유권, 통치권, 공급권을 이양한다.\n",
      "- 십일조를 통해서 내 마음을 붙들고 있는 탐욕과 욕심, 인색, 자기의존을 끊게 된다.\n",
      "- 자신의 소유도 하나님의 것이기 때문에 하나님께서 보호해주시는 것을 체험할 수 있다.\n",
      "성경에 따르면 십일조는 창고에 넣으라고 말씀하신다. 창고는 양식을 보관하고 나누어 주는 곳이다. 당\n",
      "신에게 있어 영적 양식을 주고, 하나님나라의 삶을 가르치고 성도의 교제를 나누는 곳은 어디인가? 그곳\n",
      "은 교회이다. 십일조는 창고에 넣어야 한다. 그러나 교회가 그 역할을 제대로 하지 못할 경우나 진정한\n",
      "복음을 전하지 않음에도 불구하고 어쩔 수 없이 십일조하는 것은 금해야 한다.\n",
      "말3:10 만군의 여호와가 이르노라 너희의 온전한 십일조를 창고에 들여 나의 집에 양식이 있게 하고 그것으로 나를\n",
      "시험하여 내가 하늘 문을 열고 너희에게 복을 쌓을 곳이 없도록 붓지 아니하나 보라\n",
      "# 십일조를 어디에 해야 하는가?\n",
      "십일조와 헌금을 드리는 것은 매우 신중해야 한다. 돈은 소중하다. 그 돈이 가는 곳에 마음이 간다. 돈을\n",
      "흘려 보내는 것은 능력을 주는 것이다. 하나님나라를 확장시키지 않는 곳에 돈을 흘려보내는 것은 당신\n",
      "이 하나님의 일을 방해하는 것과 마찬가지이다.\n",
      "당신이 영적 양식을 먹는 곳에 물질을 흘려보내라. 그곳에서 영적 양식을 먹으면서도 물질을 흘려보내지 않\n",
      "는 것은 축복받을 수 없다. 그곳이 하나님의 영적 양식을 주는 곳이라고 생각하고 실제적으로 양식을 공\n",
      "급받으면서, 영적 양식을 공급해주는 곳으로 물질을 흘려 보내지 않는 것은 이율배반적이다. 대부분의\n",
      "경우에 있어서 영적 양식을 보관하고 나누어주는 창고는 지금 자신이 다니는 교회일 것이다. 영적 양식\n",
      "을 먹는 곳을 잘 정해야 한다.\n",
      "마 6:21 네 보물 있는 그 곳에는 네 마음도 있느니라\n",
      "# ▶ 십일조를 한군데 다해야 하는가?\n",
      "\n",
      "Source: 4a414ed1-67ae-49f6-acf5-a92ad7a3c206_31_kbs_cleaned.pdf\n",
      "\n",
      "---\n",
      "\n",
      "### Result 6 (Page: 113)\n",
      "\n",
      "담당함으로 인하여 친히 뱀(죄가 되고 저주를 받을 것)이 될 것이라는 것을 의미하고 있다.\n",
      "갈3:13 그리스도께서 우리를 위하여 저주를 받은바 되사 율법의 저주에서 우리를 속량하셨으니 기록된 바 나무에\n",
      "달린 자마다 저주 아래에 있는 자라 하였음이라\n",
      "고후5:21 하나님이 죄를 알지도 못하신 이를 우리를 대신하여 죄로 삼으신 것은 우리로 하여금 그 안에서 하나님의\n",
      "의가 되게 하려 하심이라\n",
      "뱀의 표징은 죄가 되고 저주받으실 예수님을 나타내며 결국 그것은 바로 우리의 타락한 실체이고 우리\n",
      "의 형상인 것이다. 십자가는 그 자체로 결코 아름다운 것이 아니다. 우리는 흔히 십자가를 볼 때 우리를\n",
      "위해서 죽으신 그분의 놀랍고 아름다운 사랑만을 생각한다. 그러나 우리는 예수님이 행하신 일을 보기\n",
      "전에 먼저 내가 이 땅에서 행한 일을 먼저 보아야 한다. 즉 먼저 그 십자가에서 마귀의 형상인 뱀을 보아\n",
      "야 한다.\n",
      "우리가 바로 마귀의 형상과 모양대로 살았던 것이다. 그분이 우리 대신에 뱀이 된 것을 보고, 우리가 그\n",
      "십자가에 우리 자아를 못 박을 때 비로소 죽음까지 불사한 그분의 아가페 사랑을 체험할 수 있게 된다.\n",
      "그 죽음 없이 십자가 앞에서 느끼는 인간적인 사랑은 예수님께서 우리에게 주시고자 하는 아가페 사랑\n",
      "이 될 수 없다.\n",
      "사53:2-3 (2) 그는 주 앞에서 자라나기를 연한 순 같고 마른 땅에서 나온 뿌리 같아서 고운 모양도 없고 풍채도 없\n",
      "은즉 우리가 보기에 흠모할 만한 아름다운 것이 없도다 (3) 그는 멸시를 받아 사람들에게 버림받았으며 간고를 많이\n",
      "겪었으며 질고를 아는 자라 마치 사람들이 그에게서 얼굴을 가리는 것 같이 멸시를 당하였고 우리도 그를 귀히 여기\n",
      "지 아니하였도다\n",
      "우리는 십자가를 볼 때, 1) 우리의 죄와 저주를 친히 담당하신 예수 그리스도를 보아야 하며, 2) 예수님의\n",
      "아름다운 모습이 아니라 죄와 저주로 인한 마귀의 형상을 보아야 하며, 3) 뱀이 바로 모든 죄와 저주를 지\n",
      "\n",
      "Source: 4a414ed1-67ae-49f6-acf5-a92ad7a3c206_31_kbs_cleaned.pdf\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36magent\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "본문에서 십일조와 관련된 말씀은 다음과 같습니다:\n",
      "\n",
      "1. 하나님은 우리의 물질이 필요한 것이 아니라, 우리의 삶이 풍성해지고 축복받기를 원하신다. 십일조는 하나님을 신뢰하는 믿음을 실제적으로 증명하는 행위이다. (고전 4:7)\n",
      "\n",
      "2. 십일조를 통해 물질의 대속함을 받는다. 번 돈의 10%를 하나님께 드림으로 나머지 90%를 하나님께서 관리해 주시는 축복을 받는다. (출 13:2, 레 27:30, 잠 3:9-10)\n",
      "\n",
      "3. 십일조는 율법 아래 있을 때만 하는 것이 아니라, 아브라함이 멜기세덱에게 십일조를 드린 것처럼 성경적 원리이다. (창 14:18-20, 히 7:1-10)\n",
      "\n",
      "4. 십일조는 의무감이 아니라 믿음의 행위이며, 하나님의 자녀로서 모든 재물이 주님의 것임을 고백하는 것이다.\n",
      "\n",
      "5. 십일조는 마음에 물질보다 하나님을 최우선 순위에 놓게 하고, 소유권, 통치권, 공급권을 하나님께 이양하는 행위이다. 또한 탐욕과 욕심, 인색, 자기의존을 끊게 한다.\n",
      "\n",
      "6. 십일조는 창고에 넣어야 하며, 창고는 영적 양식을 보관하고 나누어 주는 곳으로 보통 교회를 의미한다. 십일조를 드리는 곳은 영적 양식을 공급받는 곳이어야 한다. (말 3:8-10, 마 6:21)\n",
      "\n",
      "7. 십일조를 한 곳에 다 해야 하는지, 여러 군데 나누어 내어도 되는지에 대한 질문이 있는데, 마음에 평안이 있으면 나누어 내어도 괜찮다.\n",
      "\n",
      "이 내용들은 본문 226~228쪽에 주로 나와 있습니다. 필요하시면 더 구체적인 구절이나 설명도 제공해 드릴 수 있습니다."
     ]
    }
   ],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from utils import ainvoke_graph, astream_graph\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model_name=\"gpt-4.1-mini\", temperature=0, max_tokens=20000\n",
    ")\n",
    "\n",
    "async with MultiServerMCPClient(\n",
    "    {\n",
    "        \"tbb-db-schema\": {\n",
    "            # 서버의 포트와 일치해야 합니다.(8005번 포트)\n",
    "            \"url\": \"http://localhost:8006/sse\",\n",
    "            # \"url\": \"http://localhost:8008/sse\",\n",
    "            \"transport\": \"sse\",\n",
    "        }\n",
    "    }\n",
    ") as client:\n",
    "    print(client.get_tools())\n",
    "    agent = create_react_agent(model, client.get_tools())\n",
    "    # answer = await astream_graph(agent, {\"messages\": \"서울의 날씨는 어떠니?\"})\n",
    "    # answer = await astream_graph(agent, {\"messages\": \"텀블벅 스키마\"})\n",
    "    # 현재시간을 구하는 MCP\n",
    "    # answer = await astream_graph(agent, {\"messages\": \"현재 서울 시간은 몇시인가요?\"})\n",
    "\n",
    "    # kbs rag mcp 테스트\n",
    "    # answer = await astream_graph(agent, {\"messages\": \"심중에 대해 자세히 설명해 주세요.\"})\n",
    "    # answer = await astream_graph(agent, {\"messages\": \"십일조는 왜 해야하나요?\"})\n",
    "    # answer = await astream_graph(agent, {\"messages\": \"드림과 나눔 중 어느 것이 더 중요한지 본문을 인용해서 설명해 주세요.\"})\n",
    "    answer = await astream_graph(agent, {\"messages\": \"본문에서 십일조와 관련된 성경구절을 찾아주세요.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  + Exception Group Traceback (most recent call last):\n",
      "  |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3665, in run_code\n",
      "  |     await eval(code_obj, self.user_global_ns, self.user_ns)\n",
      "  |   File \"/var/folders/rc/w7m_bf7x39bfb8wclzs_8xsr0000gn/T/ipykernel_44989/1848944460.py\", line 10, in <module>\n",
      "  |     async with MultiServerMCPClient(\n",
      "  |                ^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/langchain_mcp_adapters/client.py\", line 357, in __aenter__\n",
      "  |     await self.connect_to_server(server_name, **connection)\n",
      "  |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/langchain_mcp_adapters/client.py\", line 170, in connect_to_server\n",
      "  |     await self.connect_to_server_via_sse(\n",
      "  |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/langchain_mcp_adapters/client.py\", line 276, in connect_to_server_via_sse\n",
      "  |     sse_transport = await self.exit_stack.enter_async_context(\n",
      "  |                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 659, in enter_async_context\n",
      "  |     result = await _enter(cm)\n",
      "  |              ^^^^^^^^^^^^^^^^\n",
      "  |   File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 210, in __aenter__\n",
      "  |     return await anext(self.gen)\n",
      "  |            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/mcp/client/sse.py\", line 43, in sse_client\n",
      "  |     async with anyio.create_task_group() as tg:\n",
      "  |                ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 772, in __aexit__\n",
      "  |     raise BaseExceptionGroup(\n",
      "  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "  +-+---------------- 1 ----------------\n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n",
      "    |     yield\n",
      "    |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 394, in handle_async_request\n",
      "    |     resp = await self._pool.handle_async_request(req)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py\", line 256, in handle_async_request\n",
      "    |     raise exc from None\n",
      "    |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py\", line 236, in handle_async_request\n",
      "    |     response = await connection.handle_async_request(\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py\", line 101, in handle_async_request\n",
      "    |     raise exc\n",
      "    |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py\", line 78, in handle_async_request\n",
      "    |     stream = await self._connect(request)\n",
      "    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/httpcore/_async/connection.py\", line 124, in _connect\n",
      "    |     stream = await self._network_backend.connect_tcp(**kwargs)\n",
      "    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/httpcore/_backends/auto.py\", line 31, in connect_tcp\n",
      "    |     return await self._backend.connect_tcp(\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py\", line 113, in connect_tcp\n",
      "    |     with map_exceptions(exc_map):\n",
      "    |          ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 158, in __exit__\n",
      "    |     self.gen.throw(value)\n",
      "    |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n",
      "    |     raise to_exc(exc) from exc\n",
      "    | httpcore.ConnectError: All connection attempts failed\n",
      "    | \n",
      "    | The above exception was the direct cause of the following exception:\n",
      "    | \n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/mcp/client/sse.py\", line 47, in sse_client\n",
      "    |     async with aconnect_sse(\n",
      "    |                ^^^^^^^^^^^^^\n",
      "    |   File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 210, in __aenter__\n",
      "    |     return await anext(self.gen)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/httpx_sse/_api.py\", line 69, in aconnect_sse\n",
      "    |     async with client.stream(method, url, headers=headers, **kwargs) as response:\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 210, in __aenter__\n",
      "    |     return await anext(self.gen)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1583, in stream\n",
      "    |     response = await self.send(\n",
      "    |                ^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1629, in send\n",
      "    |     response = await self._send_handling_auth(\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1657, in _send_handling_auth\n",
      "    |     response = await self._send_handling_redirects(\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1694, in _send_handling_redirects\n",
      "    |     response = await self._send_single_request(request)\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/httpx/_client.py\", line 1730, in _send_single_request\n",
      "    |     response = await transport.handle_async_request(request)\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 393, in handle_async_request\n",
      "    |     with map_httpcore_exceptions():\n",
      "    |          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/opt/homebrew/Cellar/python@3.12/3.12.9/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py\", line 158, in __exit__\n",
      "    |     self.gen.throw(value)\n",
      "    |   File \"/Users/rock/Documents/dev-codes/python-projects/langgraph-mcp-agents/.venv/lib/python3.12/site-packages/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n",
      "    |     raise mapped_exc(message) from exc\n",
      "    | httpx.ConnectError: All connection attempts failed\n",
      "    +------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from utils import ainvoke_graph, astream_graph\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model_name=\"gpt-4.1\", temperature=0, max_tokens=20000\n",
    ")\n",
    "\n",
    "async with MultiServerMCPClient(\n",
    "    {\n",
    "        \"weather\": {\n",
    "            # 서버의 포트와 일치해야 합니다.(8005번 포트)\n",
    "            \"url\": \"http://localhost:8005/sse\",\n",
    "            \"transport\": \"sse\",\n",
    "        }\n",
    "    }\n",
    ") as client:\n",
    "    print(client.get_tools())\n",
    "    agent = create_react_agent(model, client.get_tools())\n",
    "    answer = await astream_graph(agent, {\"messages\": \"서울의 현재 시간은?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음의 경우에는 session 이 닫혔기 때문에 도구에 접근할 수 없는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36magent\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "현재 저는 실시간 인터넷 접속이 불가능하여 최신 서울의 날씨 정보를 직접 제공할 수 없습니다. 하지만 일반적으로 6월의 서울은 초여름 날씨로, 평균 기온은 20~28도 사이이며, 습도가 높고 간혹 소나기가 내릴 수 있습니다.\n",
      "\n",
      "정확한 오늘의 날씨를 확인하려면 네이버, 다음, 기상청, 또는 스마트폰의 날씨 앱을 참고해 주세요! 필요하다면 서울의 계절별 기후 특징이나 여행 팁도 알려드릴 수 있습니다."
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node': 'agent',\n",
       " 'content': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_a1102cf978'}, id='run-40cb5132-6563-454d-83ec-cc4fb6567166'),\n",
       " 'metadata': {'langgraph_step': 1,\n",
       "  'langgraph_node': 'agent',\n",
       "  'langgraph_triggers': ('branch:to:agent',),\n",
       "  'langgraph_path': ('__pregel_pull', 'agent'),\n",
       "  'langgraph_checkpoint_ns': 'agent:b4bf4fc1-cd57-415d-f75a-adbb5b07bcb2',\n",
       "  'checkpoint_ns': 'agent:b4bf4fc1-cd57-415d-f75a-adbb5b07bcb2',\n",
       "  'ls_provider': 'openai',\n",
       "  'ls_model_name': 'gpt-4.1',\n",
       "  'ls_model_type': 'chat',\n",
       "  'ls_temperature': 0.0,\n",
       "  'ls_max_tokens': 20000}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await astream_graph(agent, {\"messages\": \"서울의 현재 시간은?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 그럼 Async Session 을 유지하며 도구에 접근하는 방식으로 변경해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StructuredTool(name='get_weather', description='\\n    Get current weather information for the specified location.\\n\\n    This function simulates a weather service by returning a fixed response.\\n    In a production environment, this would connect to a real weather API.\\n\\n    Args:\\n        location (str): The name of the location (city, region, etc.) to get weather for\\n\\n    Returns:\\n        str: A string containing the weather information for the specified location\\n    ', args_schema={'properties': {'location': {'title': 'Location', 'type': 'string'}}, 'required': ['location'], 'title': 'get_weatherArguments', 'type': 'object'}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x10b898fe0>)]\n"
     ]
    }
   ],
   "source": [
    "# 1. 클라이언트 생성\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"weather\": {\n",
    "            \"url\": \"http://localhost:8005/sse\",\n",
    "            \"transport\": \"sse\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# 2. 명시적으로 연결 초기화 (이 부분이 필요함)\n",
    "# 초기화\n",
    "await client.__aenter__()\n",
    "\n",
    "# 이제 도구가 로드됨\n",
    "print(client.get_tools())  # 도구가 표시됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "langgraph 의 에이전트를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에이전트 생성\n",
    "agent = create_react_agent(model, client.get_tools())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래프를 실행하여 결과를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36magent\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mtools\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "It's always Sunny in 서울\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36magent\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "서울의 날씨는 항상 맑음입니다! 오늘도 화창한 하루를 보내실 수 있겠네요."
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node': 'agent',\n",
       " 'content': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_cf18407276'}, id='run-3f2f12d4-9aef-4379-9d53-3a83e3b18046'),\n",
       " 'metadata': {'langgraph_step': 3,\n",
       "  'langgraph_node': 'agent',\n",
       "  'langgraph_triggers': ('branch:to:agent',),\n",
       "  'langgraph_path': ('__pregel_pull', 'agent'),\n",
       "  'langgraph_checkpoint_ns': 'agent:bfbec7ec-c929-5bdd-1650-5caa310cdd02',\n",
       "  'checkpoint_ns': 'agent:bfbec7ec-c929-5bdd-1650-5caa310cdd02',\n",
       "  'ls_provider': 'openai',\n",
       "  'ls_model_name': 'gpt-4.1',\n",
       "  'ls_model_type': 'chat',\n",
       "  'ls_temperature': 0.0,\n",
       "  'ls_max_tokens': 20000}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await astream_graph(agent, {\"messages\": \"서울의 현재 시간은?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stdio 통신 방식\n",
    "\n",
    "Stdio 통신 방식은 로컬 환경에서 사용하기 위해 사용합니다.\n",
    "\n",
    "- 통신을 위해 표준 입력/출력 사용\n",
    "\n",
    "참고: 아래의 python 경로는 수정하세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from utils import ainvoke_graph, astream_graph\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model_name=\"gpt-4.1\", temperature=0, max_tokens=20000\n",
    ")\n",
    "\n",
    "# StdIO 서버 파라미터 설정\n",
    "# - command: Python 인터프리터 경로\n",
    "# - args: 실행할 MCP 서버 스크립트\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"./.venv/bin/python\",\n",
    "    # args=[\"mcp_server_local.py\"],\n",
    "    args=[\"./resources/mcp_rag_help_center/mcp_server.py\"],\n",
    ")\n",
    "\n",
    "# StdIO 클라이언트를 사용하여 서버와 통신\n",
    "async with stdio_client(server_params) as (read, write):\n",
    "    # 클라이언트 세션 생성\n",
    "    async with ClientSession(read, write) as session:\n",
    "        # 연결 초기화\n",
    "        await session.initialize()\n",
    "\n",
    "        # MCP 도구 로드\n",
    "        tools = await load_mcp_tools(session)\n",
    "        print(tools)\n",
    "\n",
    "        # 에이전트 생성\n",
    "        agent = create_react_agent(model, tools)\n",
    "\n",
    "        # 에이전트 응답 스트리밍\n",
    "        # await astream_graph(agent, {\"messages\": \"서울의 날씨는 어떠니?\"})\n",
    "        await astream_graph(agent, {\"messages\": \"창작자에 대해 설명해줘\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG 를 구축한 MCP 서버 사용\n",
    "\n",
    "- 파일: `mcp_server_rag.py`\n",
    "\n",
    "사전에 langchain 으로 구축한 `mcp_server_rag.py` 파일을 사용합니다.\n",
    "\n",
    "stdio 통신 방식으로 도구에 대한 정보를 가져옵니다. 여기서 도구는 `retriever` 도구를 가져오게 되며, 이 도구는 `mcp_server_rag.py` 에서 정의된 도구입니다. 이 파일은 사전에 서버에서 실행되지 **않아도** 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36magent\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mtools\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "정책･법제\n",
      "기업･산업\n",
      "기술･연구\n",
      "인력･교육\n",
      "3\n",
      "파리 AI 행동 정상회의, AI의 공공성·지속가능성과 함께 규제 완화 논의\n",
      "n 파리 AI 행동 정상회의의 주요 결과물로 포용적이고 지속가능한 AI를 위한 선언문이 발표되었으며, \n",
      "공익을 위한 AI 프로젝트 ‘커런트 AI’도 출범\n",
      "n 파리 AI 정상회의에서는 AI 규제 완화와 투자 확대가 핵심 의제로 떠올랐으며, 마크롱 프랑스 \n",
      "대통령은 1,090억 유로 규모의 AI 인프라 민간 투자 프로젝트도 발표\n",
      "KEY Contents\n",
      "£ 미국과 영국을 제외한 60개국, 포용적이고 지속가능한 AI에 대한 선언문 발표\n",
      "n 프랑스 정부가 2025년 2월 10~11일, 파리에서 ‘AI 행동 정상회의(AI Action Summit)’를 개최\n",
      "∙이번 정상회의에는 전 세계 87개 국가에서 기업, 국제기구, 시민단체 등 총 1,000여 명이 참여해 AI 글로벌 \n",
      "거버넌스를 심도 있게 논의\n",
      "n 정상회의 주요 결과물로 한국을 포함한 60개국이 공동 참여한 ‘인류와 지구를 위한 포용적이고 \n",
      "지속가능한 AI에 대한 선언문’이 발표되었고 미국과 영국은 선언에 불참    \n",
      "∙선언문은 공익을 위한 AI 및 지속가능한 AI를 목표로 제시하고, 이를 달성하기 위한 구체적 행동으로 \n",
      "①공익을 위한 AI 플랫폼 및 인큐베이터 출범 ②환경적 지속가능성을 위한 AI 에너지 관측소 설립 \n",
      "③일자리에 대한 AI 영향 관측 네트워크를 제시\n",
      "n 이번 정상회의에서는 9개국*과 구글, 세일즈포스 등의 기업이 4억 달러를 투자해 공익을 위한 \n",
      "AI 프로젝트 ‘커런트 AI(Current AI)’도 출범\n",
      "* 나이지리아, 독일, 모로코, 스위스, 슬로베니아, 칠레, 케냐, 프랑스, 핀란드  \n",
      "∙이 프로젝트는 공익 AI 환경 조성을 목표로 △AI 훈련을 위한 고품질 공공 데이터 접근성 확대 \n",
      "△오픈소스 인프라 지원 △AI의 사회적·환경적 영향을 측정하기 위한 시스템 개발 지원을 추진\n",
      "£ 파리 AI 정상회의, AI 규제 완화 및 AI 투자 확대가 핵심 화두로 부상\n",
      "SPRi AI Brief\n",
      "2025년 3월호\n",
      "4\n",
      "EU 집행위원회, 경쟁력 강화 로드맵의 일환으로 AI 기가팩토리 구축 추진\n",
      "n EU 집행위원회가 5개년 정책 로드맵 ‘경쟁력 나침반’에 따라 혁신 격차 해소를 위한 AI 정책으로 \n",
      "‘AI 기가팩토리’와 ‘AI 적용’ 전략을 제시\n",
      "n EU 집행위원회는 파리 AI 정상회의에서 총 2천억 유로 규모의 ‘인베스트AI’ 계획의 일환으로 \n",
      "200억 유로를 투입해 4개의 AI 기가팩토리를 건설하겠다고 발표  \n",
      "KEY Contents\n",
      "£ EU 집행위원회, 5개년 정책 로드맵 하에서 AI 기가팩토리와 AI 적용 전략 추진 \n",
      "n 우르줄라 폰 데어 라이엔(Ursula von der Leyen) EU 집행위원장이 2025년 1월 29일 2기 EU \n",
      "집행부(2024년 12월 출범)의 5개년 정책 로드맵 ‘경쟁력 나침반(Competitive Compass)’을 발표\n",
      "∙EU 집행위원회는 경쟁력 제고를 위해 △혁신 격차 해소 △脫탄소화 △공급망 안보의 3개 영역을 중점 \n",
      "과제로 제시했으며, 이중 혁신 격차 해소와 관련해 AI 정책을 포함\n",
      "n EU 집행위원회는 핵심 분야의 AI 개발과 산업계 AI 도입 활성화를 위한 ‘AI 기가팩토리(AI Gigafactory)’와 \n",
      "‘AI 적용(Apply AI)’ 전략을 제안\n",
      "∙AI 기가팩토리는 입법 추진 예정인 ‘EU 클라우드 및 AI 개발법(EU Cloud and AI Development \n",
      "Act)’*을 통해 공공과 민간 자금을 활용하여 초거대 AI 모델 훈련에 특화된 대규모 데이터센터를 \n",
      "구축함으로써 EU 전역의 AI 생태계를 활성화한다는 계획\n",
      "* 고성능 연산 자원과 디지털 인프라에 대한 강력한 규제 프레임워크를 통해 클라우드와 AI 분야에서 유럽의 리더십 강화를 위한 법안\n",
      "∙AI 적용 전략은 제조업, 에너지, 자동차, 로봇공학, 제약, 항공, 금융 서비스 산업의 AI 적용을 촉진하고 \n",
      "보건과 사법 등의 공공서비스 개선을 추진\n",
      "CONTENTS\n",
      "정책･법제\n",
      "∙미국 국가AI자문위원회, 트럼프 행정부 대상 AI 정책 보고서 발간\n",
      "2\n",
      "∙파리 AI 행동 정상회의, AI의 공공성·지속가능성과 함께 규제 완화 논의\n",
      "3\n",
      "∙EU 집행위원회, 경쟁력 강화 로드맵의 일환으로 AI 기가팩토리 구축 추진\n",
      "4\n",
      "∙EU 집행위원회, 2025년 업무 프로그램에서 ‘AI 책임 지침’ 철회 계획 발표\n",
      "5\n",
      "∙영국, AI안전연구소의 명칭 AI보안연구소로 변경하고 앤스로픽과 AI 협력 발표\n",
      "6\n",
      "기업･산업\n",
      "∙구글, 성능 개선한 ‘제미나이 2.0’ 제품군 확대 출시\n",
      "8\n",
      "∙ 오픈AI, 심층 조사를 수행하는 에이전트 ‘딥 리서치’ 공개\n",
      "9\n",
      "∙오픈AI, GPT-5 출시 로드맵 발표 및 GPT-4.5 프리뷰 공개\n",
      "10\n",
      "∙xAI, 최신 AI 모델 ‘그록 3’ 프리뷰 출시\n",
      "11\n",
      "∙퍼플렉시티, 심층 조사와 분석을 수행하는 ‘딥 리서치’ 무료 출시\n",
      "12\n",
      "∙알리바바, 딥시크 V3 능가하는 성능의 ‘큐원2.5-Max’ 출시\n",
      "13\n",
      "∙아크 인베스트, 2025년 혁신 기술 중 하나로 AI 에이전트 선정\n",
      "14\n",
      "기술･연구\n",
      "∙상하이교통⼤ 연구진, 소량의 고품질 데이터를 활용한 추론 AI 모델 개발\n",
      "16\n",
      "∙스탠포드⼤와 워싱턴⼤ 연구진, 저비용으로 고성능 추론 AI 모델 개발\n",
      "17\n",
      "∙바이트댄스, 인물 움직임을 생성하는 AI 모델 ‘옴니휴먼-1’ 개발\n",
      "18\n",
      "∙AI안전센터와 스케일 AI, 고난도 벤치마크 ‘HLE’ 공개\n",
      "19\n",
      "인력･교육 \n",
      "∙딜로이트 조사 결과, 전 세계 기업들은 점진적으로 AI 도입 확대\n",
      "21\n",
      "∙마이크로소프트 연구 결과, 생성 AI 신뢰할수록 비판적 사고 감소\n",
      "22\n",
      "∙앤스로픽, AI가 노동시장에 미치는 영향을 분석한 ‘앤스로픽 경제 지수’ 공개 \n",
      "23\n",
      "∙앤스로픽을 비롯한 여러 기업들, 입사 지원 시 AI 도구 사용금지 요구\n",
      "24\n",
      "주요행사일정\n",
      "25\n",
      "| 2025년 3월호 |\n",
      "SPRi AI Brief\n",
      "2025년 3월호\n",
      "2\n",
      "미국 국가AI자문위원회, 트럼프 행정부 대상 AI 정책 보고서 발간\n",
      "n AI 정책에 관하여 대통령과 연방 정부에 자문을 제공하는 미국 국가AI자문위원회(NAIAC)가 \n",
      "트럼프 행정부가 우선적으로 추진해야 할 AI 정책을 정리한 보고서를 발표\n",
      "n 보고서는 정책 우선순위로 △노동력 △AI 인식과 리터러시 △교육 △과학 △보건 △정부 \n",
      "△중소기업 지원 △AI 거버넌스 △미국 시민 △법 집행을 제시\n",
      "KEY Contents\n",
      "£ 미국 국가AI자문위원회, 미국의 AI 주도권 유지를 위한 10대 정책 우선순위 제시\n",
      "n 미국 국가AI자문위원회는 2025년 1월 28일 미국의 기술 주도권 유지를 위해 트럼프 행정부가 추진\n",
      "해야 할 AI 중점 분야를 제시한 보고서를 발표하고, 정책 우선순위로 △노동력 △AI 인식과 리터러시 \n",
      "△교육 △과학 △보건 △정부 △중소기업 지원 △AI 거버넌스 △미국 시민 △법 집행을 선정\n",
      "∙(노동력) AI가 노동시장에 미치는 영향에 대응해 연방 정부와 주·지방 정부 간 협력을 강화하고 AI를 비롯한 \n",
      "기술 개발 지원 등 AI로 실직 위험에 처한 근로자 지원 전략을 마련\n",
      "∙(AI 인식과 리터러시) AI의 광범위한 도입을 위해 전국 규모의 AI 인식 제고 캠페인을 시행하고 AI 기초 \n",
      "교육, 전문과정, AI 자격증 과정과 같은 교육 프로그램을 강화\n",
      "∙(교육) 교육 환경에 특화된 AI 위험관리 프레임워크를 개발하고 AI 교육 과정과 AI 도구 개발 해커톤을 개최\n",
      "∙(과학) 중요한 사회 문제 해결을 위해 AI 적용을 확대하고, 과학 분야의 AI 연구에 대한 자금 우선순위를 \n",
      "설정하며 지원을 강화하는 전략을 추진\n",
      "∙(보건) 백악관 과학기술위원회(NSTC) 산하 AI특별위원회 내에 개인 건강정보의 안전하고 책임 있는 \n",
      "사용을 담당할 소위원회를 신설 \n",
      "∙(정부) AI로 정부의 운영 효율성을 향상하기 위해 국가AI이니셔티브 사무국에 인력과 자원을 충원하고 AI \n",
      "모델 평가 프레임워크를 구축\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36magent\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "파리 AI 행동 정상회의(AI Action Summit)는 2025년 2월 10~11일 프랑스 파리에서 개최된 글로벌 AI 정상회의입니다. 이 회의에는 전 세계 87개국에서 정부, 기업, 국제기구, 시민단체 등 약 1,000여 명이 참여해 AI의 글로벌 거버넌스, 공공성, 지속가능성, 규제 완화, 투자 확대 등을 논의했습니다.\n",
      "\n",
      "주요 내용과 결과는 다음과 같습니다:\n",
      "\n",
      "1. **포용적이고 지속가능한 AI 선언문 발표**\n",
      "   - 한국을 포함한 60개국이 ‘인류와 지구를 위한 포용적이고 지속가능한 AI에 대한 선언문’을 공동 발표했습니다(미국, 영국은 불참).\n",
      "   - 선언문은 공익을 위한 AI, 지속가능한 AI를 목표로 하며, 구체적 행동으로\n",
      "     1) 공익을 위한 AI 플랫폼 및 인큐베이터 출범,\n",
      "     2) 환경적 지속가능성을 위한 AI 에너지 관측소 설립,\n",
      "     3) 일자리에 대한 AI 영향 관측 네트워크 구축 등을 제시했습니다.\n",
      "\n",
      "2. **공익 AI 프로젝트 ‘커런트 AI(Current AI)’ 출범**\n",
      "   - 9개국(프랑스, 독일, 핀란드, 스위스, 슬로베니아, 케냐, 나이지리아, 모로코, 칠레)과 구글, 세일즈포스 등 글로벌 기업이 4억 달러를 투자해 공익 AI 프로젝트를 시작했습니다.\n",
      "   - 이 프로젝트는 고품질 공공 데이터 접근성 확대, 오픈소스 인프라 지원, AI의 사회적·환경적 영향 측정 시스템 개발 등을 목표로 합니다.\n",
      "\n",
      "3. **AI 규제 완화 및 투자 확대 논의**\n",
      "   - 마크롱 프랑스 대통령은 1,090억 유로 규모의 AI 인프라 민간 투자 프로젝트를 발표했습니다.\n",
      "   - AI 규제 완화와 투자 확대가 핵심 의제로 부상했습니다.\n",
      "\n",
      "4. **EU의 AI 정책 발표**\n",
      "   - EU 집행위원회는 2,000억 유로 규모의 ‘인베스트AI’ 계획의 일환으로 200억 유로를 투입해 4개의 AI 기가팩토리(대규모 AI 데이터센터)를 건설하겠다고 발표했습니다.\n",
      "   - AI 적용 전략을 통해 제조업, 에너지, 자동차, 로봇공학, 제약, 항공, 금융 서비스 등 다양한 산업에 AI 도입을 촉진할 계획입니다.\n",
      "\n",
      "**요약**: 파리 AI 행동 정상회의는 AI의 공공성, 지속가능성, 규제 완화, 투자 확대를 논의하는 글로벌 협력의 장으로, 포용적 AI 선언문과 공익 AI 프로젝트 출범 등 실질적 결과를 도출한 것이 특징입니다."
     ]
    }
   ],
   "source": [
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from utils import astream_graph\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model_name=\"gpt-4.1\", temperature=0, max_tokens=20000\n",
    ")\n",
    "\n",
    "# RAG 서버를 위한 StdIO 서버 파라미터 설정\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"./.venv/bin/python\",\n",
    "    args=[\"./mcp_server_rag.py\"],\n",
    ")\n",
    "\n",
    "# StdIO 클라이언트를 사용하여 RAG 서버와 통신\n",
    "async with stdio_client(server_params) as (read, write):\n",
    "    # 클라이언트 세션 생성\n",
    "    async with ClientSession(read, write) as session:\n",
    "        # 연결 초기화\n",
    "        await session.initialize()\n",
    "\n",
    "        # MCP 도구 로드 (여기서는 retriever 도구)\n",
    "        tools = await load_mcp_tools(session)\n",
    "\n",
    "        # 에이전트 생성 및 실행\n",
    "        agent = create_react_agent(model, tools)\n",
    "\n",
    "        # 에이전트 응답 스트리밍\n",
    "        await astream_graph(\n",
    "            agent, {\"messages\": \"파리 AI 행동 정상회의에 대해 설명해줘\"}\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSE 방식과 StdIO 방식 혼합 사용\n",
    "\n",
    "- 파일: `mcp_server_rag.py` 는 StdIO 방식으로 통신\n",
    "- `langchain-dev-docs` 는 SSE 방식으로 통신\n",
    "\n",
    "SSE 방식과 StdIO 방식을 혼합하여 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model_name=\"gpt-4.1\", temperature=0, max_tokens=20000\n",
    ")\n",
    "\n",
    "# 1. 다중 서버 MCP 클라이언트 생성\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"document-retriever\": {\n",
    "            \"command\": \"./.venv/bin/python\",\n",
    "            # mcp_server_rag.py 파일의 절대 경로로 업데이트해야 합니다\n",
    "            \"args\": [\"./mcp_server_rag.py\"],\n",
    "            # stdio 방식으로 통신 (표준 입출력 사용)\n",
    "            \"transport\": \"stdio\",\n",
    "        },\n",
    "        \"langchain-dev-docs\": {\n",
    "            # SSE 서버가 실행 중인지 확인하세요\n",
    "            \"url\": \"https://teddynote.io/mcp/langchain/sse\",\n",
    "            # SSE(Server-Sent Events) 방식으로 통신\n",
    "            \"transport\": \"sse\",\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# 2. 비동기 컨텍스트 매니저를 통한 명시적 연결 초기화\n",
    "await client.__aenter__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "langgraph 의 `create_react_agent` 를 사용하여 에이전트를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "prompt = (\n",
    "    \"You are a smart agent. \"\n",
    "    \"Use `retriever` tool to search on AI related documents and answer questions.\"\n",
    "    \"Use `langchain-dev-docs` tool to search on langchain / langgraph related documents and answer questions.\"\n",
    "    \"Answer in Korean.\"\n",
    ")\n",
    "agent = create_react_agent(\n",
    "    model, client.get_tools(), prompt=prompt, checkpointer=MemorySaver()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구축해 놓은 `mcp_server_rag.py` 에서 정의한 `retriever` 도구를 사용하여 검색을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RunnableConfig(recursion_limit=30, thread_id=1)\n",
    "await astream_graph(\n",
    "    agent,\n",
    "    {\n",
    "        \"messages\": \"`retriever` 도구를 사용해서 파리 AI 행동 정상회의에 대해 설명해줘\"\n",
    "    },\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 `langchain-dev-docs` 도구를 사용하여 검색을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RunnableConfig(recursion_limit=30, thread_id=1)\n",
    "await astream_graph(\n",
    "    agent,\n",
    "    {\"messages\": \"langgraph-dev-docs 참고해서 self-rag 의 정의에 대해서 알려줘\"},\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MemorySaver` 를 사용하여 단기 기억을 유지합니다. 따라서, multi-turn 대화도 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await astream_graph(\n",
    "    agent, {\"messages\": \"이전의 내용을 bullet point 로 요약해줘\"}, config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain 에 통합된 도구 + MCP 도구\n",
    "\n",
    "여기서는 LangChain 에 통합된 도구를 기존의 MCP 로만 이루어진 도구와 함께 사용이 가능한지 테스트 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# Tavily 검색 도구를 초기화 합니다. (news 타입, 최근 3일 내 뉴스)\n",
    "tavily = TavilySearchResults(max_results=3, topic=\"news\", days=3)\n",
    "\n",
    "# 기존의 MCP 도구와 함께 사용합니다.\n",
    "tools = client.get_tools() + [tavily]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "langgraph 의 `create_react_agent` 를 사용하여 에이전트를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "# 재귀 제한 및 스레드 아이디 설정\n",
    "config = RunnableConfig(recursion_limit=30, thread_id=2)\n",
    "\n",
    "# 프롬프트 설정\n",
    "prompt = \"You are a smart agent with various tools. Answer questions in Korean.\"\n",
    "\n",
    "# 에이전트 생성\n",
    "agent = create_react_agent(model, tools, prompt=prompt, checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "새롭게 추가한 `tavily` 도구를 사용하여 검색을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await astream_graph(agent, {\"messages\": \"오늘 뉴스 찾아줘\"}, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`retriever` 도구가 원활하게 작동하는 것을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await astream_graph(\n",
    "    agent,\n",
    "    {\n",
    "        \"messages\": \"`retriever` 도구를 사용해서 삼성전자가 개발한 생성형 AI 이름을 검색해줘\"\n",
    "    },\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smithery 에서 제공하는 MCP 서버\n",
    "\n",
    "- 링크: https://smithery.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사용한 도구 목록은 아래와 같습니다.\n",
    "\n",
    "- Sequential Thinking: https://smithery.ai/server/@smithery-ai/server-sequential-thinking\n",
    "  - 구조화된 사고 프로세스를 통해 역동적이고 성찰적인 문제 해결을 위한 도구를 제공하는 MCP 서버\n",
    "- Desktop Commander: https://smithery.ai/server/@wonderwhy-er/desktop-commander\n",
    "  - 다양한 편집 기능으로 터미널 명령을 실행하고 파일을 관리하세요. 코딩, 셸 및 터미널, 작업 자동화\n",
    "\n",
    "**참고**\n",
    "\n",
    "- smithery 에서 제공하는 도구를 JSON 형식으로 가져올때, 아래의 예시처럼 `\"transport\": \"stdio\"` 로 꼭 설정해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# LLM 모델 초기화\n",
    "model = ChatAnthropic(model=\"claude-3-7-sonnet-latest\", temperature=0, max_tokens=20000)\n",
    "\n",
    "# 1. 클라이언트 생성\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"server-sequential-thinking\": {\n",
    "            \"command\": \"npx\",\n",
    "            \"args\": [\n",
    "                \"-y\",\n",
    "                \"@smithery/cli@latest\",\n",
    "                \"run\",\n",
    "                \"@smithery-ai/server-sequential-thinking\",\n",
    "                \"--key\",\n",
    "                \"89a4780a-53b7-4b7b-92e9-a29815f2669b\",\n",
    "            ],\n",
    "            \"transport\": \"stdio\",  # stdio 방식으로 통신을 추가합니다.\n",
    "        },\n",
    "        \"desktop-commander\": {\n",
    "            \"command\": \"npx\",\n",
    "            \"args\": [\n",
    "                \"-y\",\n",
    "                \"@smithery/cli@latest\",\n",
    "                \"run\",\n",
    "                \"@wonderwhy-er/desktop-commander\",\n",
    "                \"--key\",\n",
    "                \"89a4780a-53b7-4b7b-92e9-a29815f2669b\",\n",
    "            ],\n",
    "            \"transport\": \"stdio\",  # stdio 방식으로 통신을 추가합니다.\n",
    "        },\n",
    "        \"document-retriever\": {\n",
    "            \"command\": \"./.venv/bin/python\",\n",
    "            # mcp_server_rag.py 파일의 절대 경로로 업데이트해야 합니다\n",
    "            \"args\": [\"./mcp_server_rag.py\"],\n",
    "            # stdio 방식으로 통신 (표준 입출력 사용)\n",
    "            \"transport\": \"stdio\",\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# 2. 명시적으로 연결 초기화\n",
    "await client.__aenter__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "langgraph 의 `create_react_agent` 를 사용하여 에이전트를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "config = RunnableConfig(recursion_limit=30, thread_id=3)\n",
    "agent = create_react_agent(model, client.get_tools(), checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Desktop Commander` 도구를 사용하여 터미널 명령을 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await astream_graph(\n",
    "    agent,\n",
    "    {\n",
    "        \"messages\": \"현재 경로를 포함한 하위 폴더 구조를 tree 로 그려줘. 단, .venv 폴더는 제외하고 출력해줘.\"\n",
    "    },\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 `Sequential Thinking` 도구를 사용하여 비교적 복잡한 작업을 수행할 수 있는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await astream_graph(\n",
    "    agent,\n",
    "    {\n",
    "        \"messages\": (\n",
    "            \"`retriever` 도구를 사용해서 삼성전자가 개발한 생성형 AI 관련 내용을 검색하고 \"\n",
    "            \"`Sequential Thinking` 도구를 사용해서 보고서를 작성해줘.\"\n",
    "        )\n",
    "    },\n",
    "    config=config,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
